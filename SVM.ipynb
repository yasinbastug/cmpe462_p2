{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cvxopt import matrix, solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(sample_size=None):\n",
    "    # Load MNIST data\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
    "\n",
    "    # Filter digits 2, 3, 8, 9\n",
    "    mask = np.isin(y, [2, 3, 8, 9])\n",
    "    X, y = X[mask], y[mask]\n",
    "\n",
    "    # sample_size\n",
    "    if sample_size is not None:\n",
    "        X, y = X[:sample_size], y[:sample_size]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "X, y = fetch_data(2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without caching or shrinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class_labels = [2, 3, 8, 9]\n",
    "\n",
    "# SVM training function adjusted for one-vs-all strategy\n",
    "def train_one_vs_all_svm_1a(X, y, class_labels, C=1.0):\n",
    "    classifiers = []\n",
    "    m, n = X.shape\n",
    "    solvers.options['show_progress'] = True  # Enable solver progress output\n",
    "\n",
    "    for label in class_labels:\n",
    "        y_binary = np.where(y == label, 1, -1).astype(float)\n",
    "\n",
    "        N, d = X.shape\n",
    "        Q = np.zeros((d + 1 + N, d + 1 + N))  # b, w, and epsilon\n",
    "        Q[1:d+1, 1:d+1] = np.eye(d)\n",
    "        p = np.hstack([np.zeros(d + 1), C * np.ones(N)])\n",
    "        \n",
    "        G_top = np.hstack([-y_binary[:, np.newaxis], -y_binary[:, np.newaxis] * X, -np.eye(N)])\n",
    "        G_bottom = np.hstack([np.zeros((N, d + 1)), -np.eye(N)])\n",
    "        G = np.vstack([G_top, G_bottom])\n",
    "        \n",
    "        h = np.hstack([-np.ones(N), np.zeros(N)])\n",
    "        \n",
    "        # Convert to cvxopt matrices\n",
    "        Q = matrix(Q)\n",
    "        p = matrix(p)\n",
    "        G = matrix(G)\n",
    "        h = matrix(h)\n",
    "\n",
    "        # Solve the QP problem\n",
    "        solution = solvers.qp(Q, p, G, h)\n",
    "        w = np.array(solution['x']).flatten()\n",
    "        \n",
    "        classifiers.append((w[1:d+1], w[0]))\n",
    "\n",
    "    return classifiers\n",
    "\n",
    "# Prediction function for one-vs-all SVM\n",
    "def predict_one_vs_all_svm_1a(X, classifiers, class_labels):\n",
    "    # Compute the decision function for each classifier\n",
    "    predictions = np.array([np.dot(X, w) + b for w, b in classifiers])\n",
    "    # Get the index of the maximum score for each sample across classifiers\n",
    "    predicted_indices = np.argmax(predictions, axis=0)\n",
    "    # Map indices to class labels\n",
    "    return np.array([class_labels[idx] for idx in predicted_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16748, 784)\n",
      "(16748,)\n",
      "(4187, 784)\n",
      "(4187,)\n"
     ]
    }
   ],
   "source": [
    "# shapes\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of slack values to test\n",
    "penalties = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for penalty in penalties:\n",
    "    print(f\"Training with penalty value: {penalty}\")\n",
    "    # Train one-vs-all SVM\n",
    "    classifiers = train_one_vs_all_svm_1a(X_train, y_train, class_labels, C=penalty)\n",
    "\n",
    "    # Predict using one-vs-all SVM\n",
    "    y_pred = predict_one_vs_all_svm_1a(X_test, classifiers, class_labels)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(f\"Accuracy with penalty {penalty}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with shrinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27914, 784) (27914,)\n"
     ]
    }
   ],
   "source": [
    "# get these labels from the dataset\n",
    "labels = [2, 3, 8, 9]\n",
    "\n",
    "# Filter digits 2, 3, 8, 9\n",
    "\n",
    "mask = np.isin(y, labels)\n",
    "X_masked, y_masked = X[mask], y[mask]\n",
    "\n",
    "# Reduce the dataset size for debugging\n",
    "\n",
    "sample_size = 2500  # Use a smaller sample for debugging\n",
    "\n",
    "X_sample, y_sample = X_masked[:sample_size], y_masked[:sample_size]\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_sample = scaler.fit_transform(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with penalty value: 0.01\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.7203e+00  1.7005e+02  1e+04  3e+00  2e+04\n",
      " 1:  3.9163e+01 -2.8554e+02  3e+02  5e-02  3e+02\n",
      " 2:  2.9244e+01 -3.6674e+01  7e+01  8e-03  5e+01\n",
      " 3:  1.4164e+01 -8.4052e+00  2e+01  3e-03  1e+01\n",
      " 4:  5.9967e+00 -1.9613e+00  8e+00  9e-04  5e+00\n",
      " 5:  3.0410e+00 -1.1282e-01  3e+00  3e-04  1e+00\n",
      " 6:  1.4206e+00  7.3606e-01  7e-01  5e-05  3e-01\n",
      " 7:  1.1010e+00  9.1832e-01  2e-01  8e-06  5e-02\n",
      " 8:  1.0147e+00  9.6845e-01  5e-02  1e-06  8e-03\n",
      " 9:  9.8951e-01  9.8422e-01  5e-03  4e-08  2e-04\n",
      "10:  9.8661e-01  9.8642e-01  2e-04  1e-09  6e-06\n",
      "11:  9.8651e-01  9.8650e-01  4e-06  2e-11  1e-07\n",
      "12:  9.8650e-01  9.8650e-01  1e-07  5e-13  3e-09\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.0777e+00  1.9280e+02  1e+04  3e+00  1e+04\n",
      " 1:  3.8908e+01 -3.2995e+02  4e+02  6e-02  3e+02\n",
      " 2:  3.0344e+01 -3.7445e+01  7e+01  9e-03  4e+01\n",
      " 3:  1.4793e+01 -7.5367e+00  2e+01  2e-03  1e+01\n",
      " 4:  7.6596e+00 -2.1610e+00  1e+01  1e-03  5e+00\n",
      " 5:  3.7385e+00  2.0909e-01  4e+00  3e-04  1e+00\n",
      " 6:  1.9419e+00  1.1034e+00  8e-01  5e-05  3e-01\n",
      " 7:  1.5440e+00  1.3279e+00  2e-01  2e-06  1e-02\n",
      " 8:  1.4578e+00  1.3766e+00  8e-02  6e-07  3e-03\n",
      " 9:  1.4131e+00  1.4034e+00  1e-02  2e-15  1e-13\n",
      "10:  1.4079e+00  1.4074e+00  5e-04  2e-15  1e-13\n",
      "11:  1.4077e+00  1.4077e+00  2e-05  2e-15  5e-13\n",
      "12:  1.4077e+00  1.4077e+00  3e-07  2e-15  7e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1534e+00  1.9791e+02  1e+04  3e+00  1e+04\n",
      " 1:  3.8486e+01 -3.7867e+02  4e+02  8e-02  3e+02\n",
      " 2:  3.1049e+01 -4.5954e+01  8e+01  1e-02  4e+01\n",
      " 3:  1.6280e+01 -9.2442e+00  3e+01  3e-03  1e+01\n",
      " 4:  7.5939e+00 -1.6166e+00  9e+00  9e-04  4e+00\n",
      " 5:  3.0091e+00  1.0370e+00  2e+00  2e-04  7e-01\n",
      " 6:  2.1296e+00  1.5540e+00  6e-01  3e-05  1e-01\n",
      " 7:  1.9098e+00  1.6778e+00  2e-01  1e-05  4e-02\n",
      " 8:  1.7944e+00  1.7411e+00  5e-02  2e-15  4e-14\n",
      " 9:  1.7693e+00  1.7590e+00  1e-02  2e-15  2e-13\n",
      "10:  1.7637e+00  1.7631e+00  7e-04  2e-15  3e-13\n",
      "11:  1.7634e+00  1.7633e+00  3e-05  2e-15  4e-13\n",
      "12:  1.7634e+00  1.7634e+00  6e-07  2e-15  8e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.3841e+00  1.4497e+02  1e+04  3e+00  2e+04\n",
      " 1:  3.8141e+01 -3.4460e+02  4e+02  7e-02  4e+02\n",
      " 2:  2.9349e+01 -3.7291e+01  7e+01  9e-03  5e+01\n",
      " 3:  1.4252e+01 -8.6516e+00  2e+01  2e-03  1e+01\n",
      " 4:  6.6771e+00 -2.6780e+00  9e+00  9e-04  5e+00\n",
      " 5:  3.0923e+00 -5.2377e-01  4e+00  3e-04  2e+00\n",
      " 6:  1.4180e+00  3.3089e-01  1e+00  7e-05  4e-01\n",
      " 7:  9.1160e-01  5.8460e-01  3e-01  5e-06  3e-02\n",
      " 8:  7.7303e-01  6.5092e-01  1e-01  1e-06  6e-03\n",
      " 9:  7.1714e-01  6.8059e-01  4e-02  2e-07  9e-04\n",
      "10:  6.9860e-01  6.9183e-01  7e-03  5e-09  3e-05\n",
      "11:  6.9514e-01  6.9450e-01  6e-04  4e-10  2e-06\n",
      "12:  6.9480e-01  6.9477e-01  4e-05  2e-11  1e-07\n",
      "13:  6.9478e-01  6.9478e-01  5e-07  3e-13  2e-09\n",
      "Optimal solution found.\n",
      "Accuracy with penalty 0.01: 0.92\n",
      "Training with penalty value: 0.1\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.8944e+00  5.2660e+02  2e+04  3e+00  3e+03\n",
      " 1:  3.4370e+02 -7.2331e+02  1e+03  2e-01  2e+02\n",
      " 2:  1.8448e+02 -1.2003e+02  3e+02  3e-02  4e+01\n",
      " 3:  7.4730e+01 -3.5184e+01  1e+02  1e-02  1e+01\n",
      " 4:  3.2889e+01 -1.2809e+01  5e+01  4e-03  5e+00\n",
      " 5:  1.6071e+01 -4.3714e+00  2e+01  2e-03  2e+00\n",
      " 6:  5.0743e+00  9.9211e-01  4e+00  2e-04  3e-01\n",
      " 7:  2.6534e+00  2.1126e+00  5e-01  8e-06  9e-03\n",
      " 8:  2.4225e+00  2.2541e+00  2e-01  1e-06  1e-03\n",
      " 9:  2.3399e+00  2.3062e+00  3e-02  3e-15  1e-13\n",
      "10:  2.3216e+00  2.3199e+00  2e-03  3e-15  2e-13\n",
      "11:  2.3207e+00  2.3206e+00  6e-05  3e-15  7e-13\n",
      "12:  2.3206e+00  2.3206e+00  1e-06  3e-15  4e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.2067e+00  5.4697e+02  1e+04  3e+00  3e+03\n",
      " 1:  3.4373e+02 -6.4947e+02  1e+03  1e-01  2e+02\n",
      " 2:  1.8245e+02 -1.1945e+02  3e+02  3e-02  4e+01\n",
      " 3:  7.9615e+01 -3.5016e+01  1e+02  1e-02  1e+01\n",
      " 4:  3.9046e+01 -1.1919e+01  5e+01  4e-03  5e+00\n",
      " 5:  2.2260e+01 -3.3749e+00  3e+01  2e-03  2e+00\n",
      " 6:  9.5204e+00  2.8716e+00  7e+00  4e-04  5e-01\n",
      " 7:  5.8480e+00  4.6227e+00  1e+00  6e-05  6e-02\n",
      " 8:  5.1431e+00  4.9882e+00  2e-01  3e-06  3e-03\n",
      " 9:  5.0554e+00  5.0431e+00  1e-02  5e-08  5e-05\n",
      "10:  5.0486e+00  5.0481e+00  5e-04  2e-09  2e-06\n",
      "11:  5.0483e+00  5.0483e+00  1e-05  4e-11  4e-08\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.5874e+00  5.4930e+02  1e+04  3e+00  2e+03\n",
      " 1:  3.3868e+02 -7.1692e+02  1e+03  2e-01  1e+02\n",
      " 2:  1.9804e+02 -1.3686e+02  3e+02  4e-02  3e+01\n",
      " 3:  9.4662e+01 -4.3188e+01  1e+02  1e-02  1e+01\n",
      " 4:  4.5229e+01 -1.3503e+01  6e+01  5e-03  4e+00\n",
      " 5:  2.1090e+01 -1.0885e+00  2e+01  2e-03  1e+00\n",
      " 6:  1.0464e+01  3.9998e+00  7e+00  3e-04  3e-01\n",
      " 7:  7.1312e+00  5.5508e+00  2e+00  7e-05  6e-02\n",
      " 8:  6.2773e+00  5.9616e+00  3e-01  7e-06  6e-03\n",
      " 9:  6.1023e+00  6.0625e+00  4e-02  6e-07  5e-04\n",
      "10:  6.0789e+00  6.0773e+00  2e-03  1e-08  1e-05\n",
      "11:  6.0780e+00  6.0780e+00  3e-05  3e-10  2e-07\n",
      "12:  6.0780e+00  6.0780e+00  8e-07  5e-12  4e-09\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.6838e+00  4.9403e+02  1e+04  3e+00  4e+03\n",
      " 1:  3.3694e+02 -6.3770e+02  1e+03  1e-01  2e+02\n",
      " 2:  1.6711e+02 -1.0911e+02  3e+02  3e-02  4e+01\n",
      " 3:  6.6414e+01 -3.3076e+01  1e+02  1e-02  1e+01\n",
      " 4:  2.8779e+01 -1.1889e+01  4e+01  4e-03  5e+00\n",
      " 5:  2.0768e+01 -8.3253e+00  3e+01  2e-03  3e+00\n",
      " 6:  1.0517e+01 -2.8142e+00  1e+01  8e-04  1e+00\n",
      " 7:  3.0832e+00  5.8307e-01  3e+00  1e-04  1e-01\n",
      " 8:  1.9764e+00  1.1459e+00  8e-01  2e-05  3e-02\n",
      " 9:  1.5495e+00  1.3616e+00  2e-01  9e-07  1e-03\n",
      "10:  1.4560e+00  1.4184e+00  4e-02  1e-07  1e-04\n",
      "11:  1.4353e+00  1.4324e+00  3e-03  4e-09  5e-06\n",
      "12:  1.4337e+00  1.4336e+00  5e-05  6e-11  8e-08\n",
      "13:  1.4337e+00  1.4337e+00  8e-07  9e-13  1e-09\n",
      "Optimal solution found.\n",
      "Accuracy with penalty 0.1: 0.908\n",
      "Training with penalty value: 1\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.7359e+03  4.4350e+03  3e+04  4e+00  4e+02\n",
      " 1:  1.6976e+03 -2.0229e+03  5e+03  7e-01  6e+01\n",
      " 2:  7.6518e+02 -6.8218e+02  2e+03  2e-01  2e+01\n",
      " 3:  3.3992e+02 -2.8911e+02  8e+02  8e-02  7e+00\n",
      " 4:  2.6244e+02 -2.1268e+02  6e+02  5e-02  4e+00\n",
      " 5:  1.2487e+02 -7.4471e+01  2e+02  2e-02  1e+00\n",
      " 6:  3.7636e+01 -1.7668e+01  6e+01  4e-03  3e-01\n",
      " 7:  1.1021e+01 -1.5232e+00  1e+01  5e-04  4e-02\n",
      " 8:  5.4107e+00  1.0135e+00  5e+00  2e-04  2e-02\n",
      " 9:  3.2455e+00  2.0438e+00  1e+00  4e-15  6e-15\n",
      "10:  2.6718e+00  2.3186e+00  4e-01  3e-15  1e-14\n",
      "11:  2.4925e+00  2.4166e+00  8e-02  3e-15  2e-14\n",
      "12:  2.4481e+00  2.4443e+00  4e-03  4e-15  3e-14\n",
      "13:  2.4460e+00  2.4459e+00  7e-05  3e-15  9e-14\n",
      "14:  2.4459e+00  2.4459e+00  8e-07  4e-15  3e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.6892e+03  4.2483e+03  2e+04  4e+00  3e+02\n",
      " 1:  1.5353e+03 -1.8264e+03  5e+03  7e-01  5e+01\n",
      " 2:  7.5127e+02 -5.8363e+02  2e+03  2e-01  1e+01\n",
      " 3:  4.6353e+02 -3.6879e+02  1e+03  1e-01  8e+00\n",
      " 4:  3.0803e+02 -2.2199e+02  6e+02  5e-02  4e+00\n",
      " 5:  1.8981e+02 -1.0944e+02  3e+02  2e-02  2e+00\n",
      " 6:  7.9987e+01 -3.4076e+01  1e+02  7e-03  5e-01\n",
      " 7:  2.1485e+01 -1.9799e+00  2e+01  1e-03  8e-02\n",
      " 8:  1.0636e+01  3.3192e+00  8e+00  3e-04  2e-02\n",
      " 9:  7.2231e+00  5.1952e+00  2e+00  5e-15  1e-14\n",
      "10:  6.3134e+00  5.6532e+00  7e-01  4e-15  4e-14\n",
      "11:  5.9472e+00  5.8424e+00  1e-01  5e-15  4e-14\n",
      "12:  5.8894e+00  5.8832e+00  6e-03  4e-15  8e-14\n",
      "13:  5.8859e+00  5.8858e+00  1e-04  4e-15  6e-13\n",
      "14:  5.8858e+00  5.8858e+00  2e-06  4e-15  2e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.6783e+03  4.4597e+03  3e+04  4e+00  3e+02\n",
      " 1:  1.6753e+03 -2.1734e+03  6e+03  8e-01  4e+01\n",
      " 2:  9.2788e+02 -8.2707e+02  2e+03  3e-01  1e+01\n",
      " 3:  4.9350e+02 -4.2501e+02  1e+03  1e-01  7e+00\n",
      " 4:  2.8132e+02 -2.2596e+02  6e+02  6e-02  3e+00\n",
      " 5:  1.7775e+02 -1.0615e+02  3e+02  2e-02  1e+00\n",
      " 6:  5.3222e+01 -1.7398e+01  8e+01  4e-03  2e-01\n",
      " 7:  1.6206e+01  2.0621e+00  2e+01  6e-04  4e-02\n",
      " 8:  9.8348e+00  5.5927e+00  4e+00  1e-04  7e-03\n",
      " 9:  7.8904e+00  6.6680e+00  1e+00  4e-15  4e-14\n",
      "10:  7.2830e+00  6.9903e+00  3e-01  3e-15  7e-14\n",
      "11:  7.1174e+00  7.0999e+00  2e-02  4e-15  1e-13\n",
      "12:  7.1073e+00  7.1070e+00  4e-04  5e-15  2e-13\n",
      "13:  7.1071e+00  7.1071e+00  6e-06  4e-15  1e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.7850e+03  4.0974e+03  2e+04  4e+00  4e+02\n",
      " 1:  1.3676e+03 -1.9054e+03  5e+03  7e-01  6e+01\n",
      " 2:  6.2123e+02 -5.3547e+02  1e+03  2e-01  1e+01\n",
      " 3:  3.8174e+02 -3.1774e+02  8e+02  9e-02  8e+00\n",
      " 4:  2.4439e+02 -1.4979e+02  4e+02  3e-02  3e+00\n",
      " 5:  5.5117e+01 -2.6596e+01  9e+01  5e-03  5e-01\n",
      " 6:  1.7764e+01 -6.0632e+00  3e+01  1e-03  1e-01\n",
      " 7:  4.3018e+00  9.5004e-02  4e+00  1e-04  1e-02\n",
      " 8:  2.6033e+00  8.4739e-01  2e+00  4e-05  3e-03\n",
      " 9:  1.8309e+00  1.2281e+00  6e-01  1e-06  1e-04\n",
      "10:  1.5531e+00  1.3611e+00  2e-01  2e-15  8e-15\n",
      "11:  1.4672e+00  1.4132e+00  5e-02  3e-15  1e-14\n",
      "12:  1.4383e+00  1.4316e+00  7e-03  3e-15  2e-14\n",
      "13:  1.4345e+00  1.4343e+00  2e-04  3e-15  9e-14\n",
      "14:  1.4344e+00  1.4344e+00  3e-06  3e-15  2e-13\n",
      "Optimal solution found.\n",
      "Accuracy with penalty 1: 0.898\n",
      "Training with penalty value: 10\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.8553e+05  1.4103e+05  5e+05  2e+01  1e+02\n",
      " 1:  1.6430e+03 -3.4295e+04  1e+05  4e+00  2e+01\n",
      " 2:  2.3332e+03 -1.7589e+04  5e+04  2e+00  9e+00\n",
      " 3:  5.3422e+03 -6.1274e+03  2e+04  4e-01  2e+00\n",
      " 4:  1.2746e+03 -1.0446e+03  3e+03  7e-02  3e-01\n",
      " 5:  3.7477e+02 -2.4464e+02  8e+02  1e-02  7e-02\n",
      " 6:  4.1197e+01 -1.7769e+01  6e+01  3e-04  2e-03\n",
      " 7:  5.4874e+00 -5.8964e-01  6e+00  3e-06  2e-05\n",
      " 8:  4.5239e+00  9.1637e-01  4e+00  1e-06  5e-06\n",
      " 9:  3.6769e+00  1.5618e+00  2e+00  5e-07  2e-06\n",
      "10:  3.0552e+00  2.0739e+00  1e+00  9e-08  5e-07\n",
      "11:  2.6648e+00  2.3180e+00  3e-01  4e-15  2e-15\n",
      "12:  2.4904e+00  2.4179e+00  7e-02  3e-15  3e-15\n",
      "13:  2.4479e+00  2.4444e+00  3e-03  3e-15  3e-15\n",
      "14:  2.4460e+00  2.4459e+00  6e-05  3e-15  5e-14\n",
      "15:  2.4459e+00  2.4459e+00  7e-07  3e-15  3e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.8298e+05  1.2848e+05  4e+05  2e+01  1e+02\n",
      " 1: -8.4437e+03 -3.0870e+04  1e+05  4e+00  2e+01\n",
      " 2: -2.6971e+03 -1.8813e+04  5e+04  2e+00  9e+00\n",
      " 3:  1.9745e+03 -5.2840e+03  1e+04  5e-01  2e+00\n",
      " 4:  9.3476e+02 -1.3733e+03  4e+03  1e-01  5e-01\n",
      " 5:  4.0468e+02 -3.7440e+02  1e+03  3e-02  1e-01\n",
      " 6:  8.1916e+01 -4.6906e+01  2e+02  3e-03  1e-02\n",
      " 7:  1.5107e+01 -1.6416e+00  2e+01  2e-04  8e-04\n",
      " 8:  9.4672e+00  2.4699e+00  8e+00  7e-05  3e-04\n",
      " 9:  9.2711e+00  4.2181e+00  5e+00  9e-06  4e-05\n",
      "10:  7.0587e+00  5.2681e+00  2e+00  1e-06  6e-06\n",
      "11:  6.2712e+00  5.6719e+00  6e-01  3e-07  1e-06\n",
      "12:  5.9360e+00  5.8501e+00  9e-02  4e-15  5e-15\n",
      "13:  5.8877e+00  5.8844e+00  3e-03  5e-15  1e-14\n",
      "14:  5.8859e+00  5.8858e+00  7e-05  3e-15  7e-14\n",
      "15:  5.8858e+00  5.8858e+00  1e-06  5e-15  3e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.8238e+05  1.3958e+05  5e+05  2e+01  8e+01\n",
      " 1: -9.2572e+03 -4.1240e+04  1e+05  6e+00  2e+01\n",
      " 2: -1.2807e+03 -2.1890e+04  6e+04  2e+00  8e+00\n",
      " 3:  2.2917e+03 -7.6311e+03  2e+04  6e-01  2e+00\n",
      " 4:  9.4531e+02 -1.8744e+03  5e+03  1e-01  5e-01\n",
      " 5:  3.8066e+02 -3.3273e+02  1e+03  2e-02  8e-02\n",
      " 6:  6.8261e+01 -3.0366e+01  1e+02  1e-03  5e-03\n",
      " 7:  1.4744e+01  3.0125e-01  2e+01  1e-04  4e-04\n",
      " 8:  1.2185e+01  3.8813e+00  9e+00  3e-05  1e-04\n",
      " 9:  9.7892e+00  5.5920e+00  4e+00  7e-06  2e-05\n",
      "10:  8.1236e+00  6.5452e+00  2e+00  9e-07  3e-06\n",
      "11:  7.3416e+00  6.9576e+00  4e-01  4e-15  7e-15\n",
      "12:  7.1264e+00  7.0940e+00  3e-02  4e-15  1e-14\n",
      "13:  7.1076e+00  7.1068e+00  8e-04  4e-15  2e-14\n",
      "14:  7.1071e+00  7.1071e+00  1e-05  3e-15  8e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.8823e+05  1.2487e+05  4e+05  2e+01  1e+02\n",
      " 1: -9.7341e+03 -2.8653e+04  9e+04  4e+00  2e+01\n",
      " 2: -3.2035e+03 -1.6777e+04  4e+04  2e+00  1e+01\n",
      " 3:  2.5248e+03 -3.9344e+03  1e+04  3e-01  2e+00\n",
      " 4:  7.8337e+02 -8.9267e+02  3e+03  7e-02  4e-01\n",
      " 5:  2.2132e+02 -1.7961e+02  6e+02  1e-02  7e-02\n",
      " 6:  6.7627e+01 -3.7984e+01  1e+02  3e-03  1e-02\n",
      " 7:  6.2032e+00 -2.3819e+00  9e+00  3e-05  2e-04\n",
      " 8:  2.9030e+00  4.3390e-02  3e+00  9e-06  5e-05\n",
      " 9:  2.1104e+00  7.1079e-01  1e+00  4e-06  2e-05\n",
      "10:  1.9338e+00  1.0870e+00  9e-01  1e-06  5e-06\n",
      "11:  1.6383e+00  1.3065e+00  3e-01  1e-07  6e-07\n",
      "12:  1.4971e+00  1.3927e+00  1e-01  1e-08  5e-08\n",
      "13:  1.4476e+00  1.4241e+00  2e-02  2e-15  2e-15\n",
      "14:  1.4358e+00  1.4333e+00  3e-03  3e-15  2e-15\n",
      "15:  1.4344e+00  1.4344e+00  5e-05  3e-15  2e-14\n",
      "16:  1.4344e+00  1.4344e+00  7e-07  2e-15  2e-14\n",
      "Optimal solution found.\n",
      "Accuracy with penalty 10: 0.898\n",
      "Training with penalty value: 100\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.8672e+07  1.1194e+07  4e+07  2e+02  1e+02\n",
      " 1: -8.0602e+05 -2.4972e+06  8e+06  4e+01  2e+01\n",
      " 2: -2.9396e+05 -1.4531e+06  4e+06  2e+01  8e+00\n",
      " 3:  2.3622e+05 -4.3717e+05  1e+06  4e+00  2e+00\n",
      " 4:  6.4897e+04 -7.1706e+04  2e+05  6e-01  3e-01\n",
      " 5:  2.0255e+04 -1.6663e+04  5e+04  1e-01  6e-02\n",
      " 6:  2.4718e+03 -1.6762e+03  5e+03  6e-03  3e-03\n",
      " 7:  2.4849e+02 -2.0514e+02  5e+02  5e-05  2e-05\n",
      " 8:  3.9056e+01 -2.5828e+01  6e+01  5e-14  6e-16\n",
      " 9:  7.2129e+00 -1.8008e+00  9e+00  2e-14  9e-16\n",
      "10:  5.0231e+00  7.9198e-02  5e+00  1e-14  6e-16\n",
      "11:  4.2214e+00  1.3537e+00  3e+00  5e-15  8e-16\n",
      "12:  3.6071e+00  1.8842e+00  2e+00  4e-15  4e-16\n",
      "13:  2.8385e+00  2.2328e+00  6e-01  3e-15  9e-16\n",
      "14:  2.5676e+00  2.3671e+00  2e-01  3e-15  1e-15\n",
      "15:  2.4634e+00  2.4341e+00  3e-02  3e-15  9e-16\n",
      "16:  2.4466e+00  2.4455e+00  1e-03  3e-15  1e-15\n",
      "17:  2.4459e+00  2.4459e+00  1e-05  4e-15  9e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.8438e+07  1.0021e+07  3e+07  2e+02  8e+01\n",
      " 1: -1.4634e+06 -2.1670e+06  7e+06  4e+01  1e+01\n",
      " 2: -4.8917e+05 -1.2462e+06  3e+06  2e+01  6e+00\n",
      " 3:  1.1155e+05 -3.2351e+05  9e+05  3e+00  1e+00\n",
      " 4:  6.4928e+04 -8.1338e+04  2e+05  7e-01  3e-01\n",
      " 5:  2.0611e+04 -1.4700e+04  5e+04  1e-01  4e-02\n",
      " 6:  2.0885e+03 -1.3210e+03  3e+03  3e-04  1e-04\n",
      " 7:  2.2751e+02 -1.7475e+02  4e+02  1e-13  1e-15\n",
      " 8:  3.8756e+01 -1.9045e+01  6e+01  4e-14  1e-15\n",
      " 9:  1.3422e+01 -9.3910e-01  1e+01  1e-14  8e-16\n",
      "10:  9.5818e+00  2.3426e+00  7e+00  9e-15  4e-16\n",
      "11:  9.6387e+00  3.8402e+00  6e+00  4e-15  7e-16\n",
      "12:  7.3575e+00  5.0657e+00  2e+00  5e-15  8e-16\n",
      "13:  6.3690e+00  5.6091e+00  8e-01  4e-15  1e-15\n",
      "14:  5.9679e+00  5.8293e+00  1e-01  4e-15  2e-15\n",
      "15:  5.8927e+00  5.8810e+00  1e-02  4e-15  1e-15\n",
      "16:  5.8859e+00  5.8857e+00  2e-04  4e-15  4e-15\n",
      "17:  5.8858e+00  5.8858e+00  3e-06  4e-15  1e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.8383e+07  1.1035e+07  4e+07  2e+02  7e+01\n",
      " 1: -1.7435e+06 -3.0109e+06  1e+07  5e+01  2e+01\n",
      " 2: -3.8685e+05 -1.3775e+06  3e+06  2e+01  5e+00\n",
      " 3:  2.2181e+05 -3.3435e+05  1e+06  3e+00  9e-01\n",
      " 4:  1.0361e+05 -9.4060e+04  3e+05  7e-01  2e-01\n",
      " 5:  2.3439e+04 -1.5395e+04  5e+04  1e-01  3e-02\n",
      " 6:  1.9368e+03 -1.4845e+03  4e+03  1e-03  4e-04\n",
      " 7:  2.5493e+02 -1.9227e+02  4e+02  9e-14  1e-15\n",
      " 8:  4.3461e+01 -2.0494e+01  6e+01  4e-14  1e-15\n",
      " 9:  1.5537e+01 -3.5647e-01  2e+01  1e-14  7e-16\n",
      "10:  1.3167e+01  3.2465e+00  1e+01  6e-15  7e-16\n",
      "11:  1.0002e+01  5.2819e+00  5e+00  4e-15  6e-16\n",
      "12:  8.5586e+00  6.3895e+00  2e+00  4e-15  6e-16\n",
      "13:  7.4091e+00  6.9298e+00  5e-01  4e-15  1e-15\n",
      "14:  7.1363e+00  7.0885e+00  5e-02  4e-15  2e-15\n",
      "15:  7.1077e+00  7.1067e+00  1e-03  4e-15  8e-15\n",
      "16:  7.1071e+00  7.1071e+00  2e-05  4e-15  7e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.8919e+07  9.7323e+06  3e+07  2e+02  9e+01\n",
      " 1: -1.6455e+06 -2.0602e+06  6e+06  4e+01  2e+01\n",
      " 2: -6.6226e+05 -1.2299e+06  3e+06  2e+01  8e+00\n",
      " 3:  3.7146e+03 -2.1288e+05  6e+05  2e+00  1e+00\n",
      " 4:  1.6302e+04 -5.0122e+04  1e+05  5e-01  2e-01\n",
      " 5:  5.3550e+03 -9.5080e+03  3e+04  9e-02  4e-02\n",
      " 6:  1.7735e+03 -1.3027e+03  4e+03  8e-03  4e-03\n",
      " 7:  1.7229e+02 -1.4543e+02  3e+02  8e-05  4e-05\n",
      " 8:  2.6898e+01 -1.8502e+01  5e+01  5e-14  6e-16\n",
      " 9:  4.8630e+00 -1.5015e+00  6e+00  2e-14  9e-16\n",
      "10:  2.6615e+00  3.3561e-01  2e+00  7e-15  7e-16\n",
      "11:  2.4209e+00  8.6615e-01  2e+00  4e-15  4e-16\n",
      "12:  1.8607e+00  1.2018e+00  7e-01  3e-15  4e-16\n",
      "13:  1.6532e+00  1.3076e+00  3e-01  2e-15  7e-16\n",
      "14:  1.4873e+00  1.4012e+00  9e-02  3e-15  1e-15\n",
      "15:  1.4440e+00  1.4269e+00  2e-02  3e-15  7e-16\n",
      "16:  1.4350e+00  1.4339e+00  1e-03  3e-15  1e-15\n",
      "17:  1.4344e+00  1.4344e+00  2e-05  3e-15  8e-16\n",
      "18:  1.4344e+00  1.4344e+00  3e-07  3e-15  3e-15\n",
      "Optimal solution found.\n",
      "Accuracy with penalty 100: 0.898\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "# Load MNIST data\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
    "labels = [2, 3, 8, 9]\n",
    "\n",
    "# Filter digits 2, 3, 8, 9\n",
    "mask = np.isin(y, labels)\n",
    "X_masked, y_masked = X[mask], y[mask]\n",
    "\n",
    "# Reduce the dataset size for debugging\n",
    "sample_size = 10000  # Use a smaller sample for debugging\n",
    "X_sample, y_sample = X_masked[:sample_size], y_masked[:sample_size]\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_sample = scaler.fit_transform(X_sample)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n",
    "\n",
    "class_labels = [2, 3, 8, 9]\n",
    "\n",
    "# Cache class to manage kernel caching\n",
    "class KernelCache:\n",
    "    def __init__(self, max_size):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def get(self, key):\n",
    "        return self.cache.get(key)\n",
    "\n",
    "    def add(self, key, value):\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            self.cache.pop(next(iter(self.cache)))  # Remove least recently used\n",
    "        self.cache[key] = value\n",
    "\n",
    "# Function to compute the kernel matrix\n",
    "def compute_kernel(X1, X2):\n",
    "    return np.dot(X1, X2.T)\n",
    "\n",
    "# SVM training function with shrinking and caching\n",
    "def train_one_vs_all_svm(X, y, class_labels, C=1.0, tol=1e-5, max_iter=1000, cache_size=500):\n",
    "    classifiers = []\n",
    "    m, n = X.shape\n",
    "    solvers.options['show_progress'] = False  # Disable solver progress output\n",
    "    solvers.options['abstol'] = tol\n",
    "    solvers.options['reltol'] = tol\n",
    "    solvers.options['maxiters'] = max_iter\n",
    "\n",
    "    kernel_cache = KernelCache(max_size=cache_size)\n",
    "\n",
    "    for label in class_labels:\n",
    "        y_binary = np.where(y == label, 1, -1).astype(float)\n",
    "        N, d = X.shape\n",
    "        Q = np.zeros((d + 1 + N, d + 1 + N))  # b, w, and epsilon\n",
    "        Q[1:d+1, 1:d+1] = np.eye(d)\n",
    "        p = np.hstack([np.zeros(d + 1), C * np.ones(N)])\n",
    "        \n",
    "        G_top = np.hstack([-y_binary[:, np.newaxis], -y_binary[:, np.newaxis] * X, -np.eye(N)])\n",
    "        G_bottom = np.hstack([np.zeros((N, d + 1)), -np.eye(N)])\n",
    "        G = np.vstack([G_top, G_bottom])\n",
    "        \n",
    "        h = np.hstack([-np.ones(N), np.zeros(N)])\n",
    "        \n",
    "        # Convert to cvxopt matrices\n",
    "        Q = matrix(Q)\n",
    "        p = matrix(p)\n",
    "        G = matrix(G)\n",
    "        h = matrix(h)\n",
    "\n",
    "        # Shrinking procedure\n",
    "        active_set = np.arange(N)\n",
    "        inactive_set = np.array([], dtype=int)\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            if len(active_set) == 0:\n",
    "                break\n",
    "\n",
    "            active_indices = np.hstack([np.arange(d+1), active_set + (d+1)])\n",
    "            active_indices = np.array(active_indices, dtype=int)\n",
    "\n",
    "            active_Q = np.zeros((len(active_indices), len(active_indices)))\n",
    "            for i, idx_i in enumerate(active_indices):\n",
    "                for j, idx_j in enumerate(active_indices):\n",
    "                    cache_key = (idx_i, idx_j)\n",
    "                    if kernel_cache.get(cache_key) is None:\n",
    "                        if idx_i < d+1 or idx_j < d+1:\n",
    "                            value = Q[idx_i, idx_j]\n",
    "                        else:\n",
    "                            X_i = X[(idx_i - (d+1)) % N]\n",
    "                            X_j = X[(idx_j - (d+1)) % N]\n",
    "                            value = compute_kernel(X_i[np.newaxis, :], X_j[np.newaxis, :])[0, 0]\n",
    "                        kernel_cache.add(cache_key, value)\n",
    "                    active_Q[i, j] = kernel_cache.get(cache_key)\n",
    "\n",
    "            active_Q = matrix(active_Q)\n",
    "            active_p = np.array(p)[active_indices]\n",
    "            active_p = matrix(active_p)\n",
    "            active_G = np.array(G)[:len(active_set)*2, :][:, active_indices]\n",
    "            active_G = matrix(active_G)\n",
    "            active_h = np.array(h)[:len(active_set)*2]\n",
    "            active_h = matrix(active_h)\n",
    "\n",
    "            solution = solvers.qp(active_Q, active_p, active_G, active_h)\n",
    "            w = np.array(solution['x']).flatten()\n",
    "\n",
    "            # Check for optimality and update active/inactive sets\n",
    "            new_active_set = []\n",
    "            for i in active_set:\n",
    "                if abs(w[d+1+i]) >= tol:\n",
    "                    new_active_set.append(i)\n",
    "                else:\n",
    "                    inactive_set = np.append(inactive_set, i)\n",
    "            active_set = np.array(new_active_set)\n",
    "\n",
    "            if len(active_set) == len(new_active_set):\n",
    "                break  # No change in the active set, stop iterating\n",
    "\n",
    "        classifiers.append((w[1:d+1], w[0]))\n",
    "\n",
    "    return classifiers\n",
    "\n",
    "# Prediction function for one-vs-all SVM\n",
    "def predict_one_vs_all_svm(X, classifiers, class_labels):\n",
    "    # Compute the decision function for each classifier\n",
    "    predictions = np.array([np.dot(X, w) + b for w, b in classifiers])\n",
    "    # Get the index of the maximum score for each sample across classifiers\n",
    "    predicted_indices = np.argmax(predictions, axis=0)\n",
    "    # Map indices to class labels\n",
    "    return np.array([class_labels[idx] for idx in predicted_indices])\n",
    "\n",
    "# Define a range of slack values to test\n",
    "penalties = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for penalty in penalties:\n",
    "    print(f\"Training with penalty value: {penalty}\")\n",
    "    # Train one-vs-all SVM\n",
    "    classifiers = train_one_vs_all_svm(X_train, y_train, class_labels, C=penalty)\n",
    "\n",
    "    # Predict using one-vs-all SVM\n",
    "    y_pred = predict_one_vs_all_svm(X_test, classifiers, class_labels)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(f\"Accuracy with penalty {penalty}: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with caching and shrinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "# Load MNIST data\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
    "labels = [2, 3, 8, 9]\n",
    "\n",
    "# Filter digits 2, 3, 8, 9\n",
    "mask = np.isin(y, labels)\n",
    "X_masked, y_masked = X[mask], y[mask]\n",
    "\n",
    "# Reduce the dataset size for debugging\n",
    "sample_size = 2500  # Use a smaller sample for debugging\n",
    "X_sample, y_sample = X_masked[:sample_size], y_masked[:sample_size]\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_sample = scaler.fit_transform(X_sample)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n",
    "\n",
    "class_labels = [2, 3, 8, 9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with penalty value: 0.01\n",
      "Accuracy with penalty 0.01: 0.898\n",
      "Training with penalty value: 0.1\n",
      "Accuracy with penalty 0.1: 0.898\n",
      "Training with penalty value: 1\n",
      "Accuracy with penalty 1: 0.898\n",
      "Training with penalty value: 10\n",
      "Accuracy with penalty 10: 0.898\n",
      "Training with penalty value: 100\n",
      "Accuracy with penalty 100: 0.898\n"
     ]
    }
   ],
   "source": [
    "# Cache class to manage kernel caching\n",
    "class KernelCache:\n",
    "    def __init__(self, max_size):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def get(self, key):\n",
    "        return self.cache.get(key)\n",
    "\n",
    "    def add(self, key, value):\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            self.cache.pop(next(iter(self.cache)))  # Remove least recently used\n",
    "        self.cache[key] = value\n",
    "\n",
    "# Function to compute the kernel matrix\n",
    "def compute_kernel(X1, X2):\n",
    "    return np.dot(X1, X2.T)\n",
    "\n",
    "# SVM training function with shrinking and caching\n",
    "def train_one_vs_all_svm(X, y, class_labels, C=1.0, tol=1e-5, max_iter=1000, cache_size=500):\n",
    "    classifiers = []\n",
    "    m, n = X.shape\n",
    "    solvers.options['show_progress'] = False  # Disable solver progress output\n",
    "    solvers.options['abstol'] = tol\n",
    "    solvers.options['reltol'] = tol\n",
    "    solvers.options['maxiters'] = max_iter\n",
    "\n",
    "    kernel_cache = KernelCache(max_size=cache_size)\n",
    "\n",
    "    for label in class_labels:\n",
    "        y_binary = np.where(y == label, 1, -1).astype(float)\n",
    "        N, d = X.shape\n",
    "        Q_np = np.zeros((d + 1 + N, d + 1 + N))  # b, w, and epsilon\n",
    "        Q_np[1:d+1, 1:d+1] = np.eye(d)\n",
    "        p = np.hstack([np.zeros(d + 1), C * np.ones(N)])\n",
    "        \n",
    "        G_top = np.hstack([-y_binary[:, np.newaxis], -y_binary[:, np.newaxis] * X, -np.eye(N)])\n",
    "        G_bottom = np.hstack([np.zeros((N, d + 1)), -np.eye(N)])\n",
    "        G = np.vstack([G_top, G_bottom])\n",
    "        \n",
    "        h = np.hstack([-np.ones(N), np.zeros(N)])\n",
    "        \n",
    "        # Convert to cvxopt matrices\n",
    "        Q = matrix(Q_np)\n",
    "        p = matrix(p)\n",
    "        G = matrix(G)\n",
    "        h = matrix(h)\n",
    "\n",
    "        # Shrinking procedure\n",
    "        active_set = np.arange(N)\n",
    "        inactive_set = np.array([], dtype=int)\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            if len(active_set) == 0:\n",
    "                break\n",
    "\n",
    "            active_indices = np.hstack([np.arange(d+1), active_set + (d+1)])\n",
    "            active_indices = np.array(active_indices, dtype=int)\n",
    "\n",
    "            active_Q = np.zeros((len(active_indices), len(active_indices)))\n",
    "            for i, idx_i in enumerate(active_indices):\n",
    "                for j, idx_j in enumerate(active_indices):\n",
    "                    cache_key = (idx_i, idx_j)\n",
    "                    if kernel_cache.get(cache_key) is None:\n",
    "                        if idx_i < d+1 or idx_j < d+1:\n",
    "                            value = Q_np[idx_i, idx_j]\n",
    "                        else:\n",
    "                            X_i = X[(idx_i - (d+1)) % N]\n",
    "                            X_j = X[(idx_j - (d+1)) % N]\n",
    "                            value = compute_kernel(X_i[np.newaxis, :], X_j[np.newaxis, :])[0, 0]\n",
    "                        kernel_cache.add(cache_key, value)\n",
    "                    active_Q[i, j] = kernel_cache.get(cache_key)\n",
    "\n",
    "            active_Q = matrix(active_Q)\n",
    "            active_p = np.array(p)[active_indices]\n",
    "            active_p = matrix(active_p)\n",
    "            active_G = np.array(G)[:len(active_set)*2, :][:, active_indices]\n",
    "            active_G = matrix(active_G)\n",
    "            active_h = np.array(h)[:len(active_set)*2]\n",
    "            active_h = matrix(active_h)\n",
    "\n",
    "            solution = solvers.qp(active_Q, active_p, active_G, active_h)\n",
    "            w = np.array(solution['x']).flatten()\n",
    "\n",
    "            # Check for optimality and update active/inactive sets\n",
    "            new_active_set = []\n",
    "            for i in active_set:\n",
    "                if abs(w[d+1+i]) >= tol:\n",
    "                    new_active_set.append(i)\n",
    "                else:\n",
    "                    inactive_set = np.append(inactive_set, i)\n",
    "            active_set = np.array(new_active_set)\n",
    "\n",
    "            if len(active_set) == len(new_active_set):\n",
    "                break  # No change in the active set, stop iterating\n",
    "\n",
    "        classifiers.append((w[1:d+1], w[0]))\n",
    "\n",
    "    return classifiers\n",
    "\n",
    "# Prediction function for one-vs-all SVM\n",
    "def predict_one_vs_all_svm(X, classifiers, class_labels):\n",
    "    # Compute the decision function for each classifier\n",
    "    predictions = np.array([np.dot(X, w) + b for w, b in classifiers])\n",
    "    # Get the index of the maximum score for each sample across classifiers\n",
    "    predicted_indices = np.argmax(predictions, axis=0)\n",
    "    # Map indices to class labels\n",
    "    return np.array([class_labels[idx] for idx in predicted_indices])\n",
    "\n",
    "# Define a range of slack values to test\n",
    "penalties = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for penalty in penalties:\n",
    "    print(f\"Training with penalty value: {penalty}\")\n",
    "    # Train one-vs-all SVM\n",
    "    classifiers = train_one_vs_all_svm(X_train, y_train, class_labels, C=penalty)\n",
    "\n",
    "    # Predict using one-vs-all SVM\n",
    "    y_pred = predict_one_vs_all_svm(X_test, classifiers, class_labels)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(f\"Accuracy with penalty {penalty}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with penalty value: 0.01\n",
      "Accuracy with penalty 0.01: 0.92\n",
      "Training with penalty value: 0.1\n",
      "Accuracy with penalty 0.1: 0.908\n",
      "Training with penalty value: 1\n",
      "Accuracy with penalty 1: 0.898\n",
      "Training with penalty value: 10\n",
      "Accuracy with penalty 10: 0.898\n",
      "Training with penalty value: 100\n",
      "Accuracy with penalty 100: 0.898\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a range of slack values to test\n",
    "penalties = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for penalty in penalties:\n",
    "    print(f\"Training with penalty value: {penalty}\")\n",
    "    # Train one-vs-all SVM\n",
    "    classifiers = train_one_vs_all_svm(X_train, y_train, class_labels, C=penalty)\n",
    "\n",
    "    # Predict using one-vs-all SVM\n",
    "    y_pred = predict_one_vs_all_svm(X_test, classifiers, class_labels)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(f\"Accuracy with penalty {penalty}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.b\n",
    "train a 4-class SVM using the scikit-learn’s soft margin primal\n",
    "SVM function with linear kernel. Please tune the hyperparameters and\n",
    "report your training and test accuracy. Compare the results with part\n",
    "(a) regarding classification accuracy and training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b1(X, y): \n",
    "    # Normalize data\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100]\n",
    "    }\n",
    "\n",
    "    # Initialize LinearSVC\n",
    "    linear_svc = LinearSVC(multi_class='ovr', max_iter=10000)\n",
    "\n",
    "    # Use GridSearchCV to find the best hyperparameters\n",
    "    grid_search = GridSearchCV(linear_svc, param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for C_value in param_grid['C']:\n",
    "        # Train the model with the specific C value\n",
    "        model = LinearSVC(C=C_value, multi_class='ovr', max_iter=10000)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on training data\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Predict on test data\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        print(f\"C = {C_value} | Training Accuracy: {train_accuracy:.3f} | Test Accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.c\n",
    "Train a 4-class non-linear SVM using one-vs-all. Please train the\n",
    "dual formulation of SVM from scratch using a quadratic programming\n",
    "solver. Please clearly write the expressions you feed to the solver. You\n",
    "may choose any kernel you like. Please tune the hyperparameters and\n",
    "report your training and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "# Load MNIST data\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
    "\n",
    "# Filter digits 2, 3, 8, 9\n",
    "mask = np.isin(y, [2, 3, 8, 9])\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Sample data\n",
    "sample_size = 2500\n",
    "X, y = X[:sample_size], y[:sample_size]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(x1, x2, gamma):\n",
    "    return np.exp(-gamma * np.linalg.norm(x1 - x2)**2)\n",
    "\n",
    "def compute_kernel_matrix(X, gamma):\n",
    "    N = X.shape[0]\n",
    "    K = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            K[i, j] = rbf_kernel(X[i], X[j], gamma)\n",
    "    return K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_vs_all_svm_1c(X, y, class_labels, C, gamma):\n",
    "    classifiers = []\n",
    "    K = compute_kernel_matrix(X, gamma)\n",
    "    N = X.shape[0]\n",
    "\n",
    "    for label in class_labels:\n",
    "        y_binary = np.where(y == label, 1, -1).astype(float)\n",
    "        \n",
    "        P = matrix(np.outer(y_binary, y_binary) * K)\n",
    "        q = matrix(-np.ones(N))\n",
    "        G = matrix(np.vstack((-np.eye(N), np.eye(N))))\n",
    "        h = matrix(np.hstack((np.zeros(N), C * np.ones(N))))\n",
    "        A = matrix(y_binary, (1, N), 'd')\n",
    "        b = matrix(0.0)\n",
    "        \n",
    "        sol = solvers.qp(P, q, G, h, A, b)\n",
    "        alphas = np.ravel(sol['x'])\n",
    "\n",
    "        support_vectors = alphas > 1e-5\n",
    "        ind = np.arange(len(alphas))[support_vectors]\n",
    "        alphas = alphas[support_vectors]\n",
    "        support_vectors_X = X[support_vectors]\n",
    "        support_vectors_y = y_binary[support_vectors]\n",
    "\n",
    "        w = np.sum(alphas[:, None] * support_vectors_y[:, None] * support_vectors_X, axis=0)\n",
    "        b = np.mean([support_vectors_y[i] - np.sum(alphas * support_vectors_y * K[ind[i], support_vectors])\n",
    "                     for i in range(len(alphas))])\n",
    "        classifiers.append((alphas, support_vectors_X, support_vectors_y, b))\n",
    "\n",
    "    return classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(X, alphas, support_vectors_X, support_vectors_y, b, gamma):\n",
    "    y_predict = np.zeros(len(X))\n",
    "    for i in range(len(X)):\n",
    "        s = 0\n",
    "        for alpha, sv_y, sv in zip(alphas, support_vectors_y, support_vectors_X):\n",
    "            s += alpha * sv_y * rbf_kernel(X[i], sv, gamma)\n",
    "        y_predict[i] = s\n",
    "    return y_predict + b\n",
    "\n",
    "def predict_one_vs_all_svm_1c(X, classifiers, class_labels, gamma):\n",
    "    predictions = np.array([project(X, alphas, support_vectors_X, support_vectors_y, b, gamma) \n",
    "                            for alphas, support_vectors_X, support_vectors_y, b in classifiers])\n",
    "    y_pred_indices = np.argmax(predictions, axis=0)\n",
    "    y_pred_labels = np.array([class_labels[idx] for idx in y_pred_indices])\n",
    "    return y_pred_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with penalty value: 0.01, gamma value: 0.01\n",
      "Accuracy with penalty 0.01 and gamma 0.01: 0.418\n",
      "Training with penalty value: 0.01, gamma value: 0.05\n",
      "Accuracy with penalty 0.01 and gamma 0.05: 0.252\n",
      "Training with penalty value: 0.01, gamma value: 0.1\n",
      "Accuracy with penalty 0.01 and gamma 0.1: 0.252\n",
      "Training with penalty value: 0.1, gamma value: 0.01\n",
      "Accuracy with penalty 0.1 and gamma 0.01: 0.696\n",
      "Training with penalty value: 0.1, gamma value: 0.05\n",
      "Accuracy with penalty 0.1 and gamma 0.05: 0.252\n",
      "Training with penalty value: 0.1, gamma value: 0.1\n",
      "Accuracy with penalty 0.1 and gamma 0.1: 0.252\n",
      "Training with penalty value: 1, gamma value: 0.01\n",
      "Accuracy with penalty 1 and gamma 0.01: 0.782\n",
      "Training with penalty value: 1, gamma value: 0.05\n",
      "Accuracy with penalty 1 and gamma 0.05: 0.26\n",
      "Training with penalty value: 1, gamma value: 0.1\n",
      "Accuracy with penalty 1 and gamma 0.1: 0.252\n",
      "Training with penalty value: 10, gamma value: 0.01\n",
      "Accuracy with penalty 10 and gamma 0.01: 0.792\n",
      "Training with penalty value: 10, gamma value: 0.05\n",
      "Accuracy with penalty 10 and gamma 0.05: 0.278\n",
      "Training with penalty value: 10, gamma value: 0.1\n",
      "Accuracy with penalty 10 and gamma 0.1: 0.252\n",
      "Training with penalty value: 100, gamma value: 0.01\n",
      "Accuracy with penalty 100 and gamma 0.01: 0.792\n",
      "Training with penalty value: 100, gamma value: 0.05\n",
      "Accuracy with penalty 100 and gamma 0.05: 0.278\n",
      "Training with penalty value: 100, gamma value: 0.1\n",
      "Accuracy with penalty 100 and gamma 0.1: 0.252\n"
     ]
    }
   ],
   "source": [
    "# Define a range of slack values to test\n",
    "penalties = [0.01, 0.1, 1, 10, 100]\n",
    "gamma_values = [0.01, 0.05, 0.1]\n",
    "\n",
    "# Train and evaluate for each combination of penalty and gamma value\n",
    "for penalty in penalties:\n",
    "    for gamma in gamma_values:\n",
    "        print(f\"Training with penalty value: {penalty}, gamma value: {gamma}\")\n",
    "        classifiers = train_one_vs_all_svm_1c(X_train, y_train, class_labels, C=penalty, gamma=gamma)\n",
    "        y_pred = predict_one_vs_all_svm_1c(X_test, classifiers, class_labels, gamma)\n",
    "\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        print(f\"Accuracy with penalty {penalty} and gamma {gamma}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.d\n",
    "Please train a 4-class SVM using the scikit-learn’s soft margin dual\n",
    "SVM function with a non-linear kernel. You may choose any kernel\n",
    "you like. Please tune the hyperparameters and report your training and\n",
    "test accuracy. Compare the results with part (c) regarding classification\n",
    "accuracy and training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Sample data\n",
    "sample_size = 2500\n",
    "X, y = X[:sample_size], y[:sample_size]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "[CV] END .................................C=0.01, gamma=0.01; total time=   3.3s\n",
      "[CV] END .................................C=0.01, gamma=0.05; total time=   3.2s\n",
      "[CV] END .................................C=0.01, gamma=0.05; total time=   3.3s\n",
      "[CV] END .................................C=0.01, gamma=0.05; total time=   3.3s\n",
      "[CV] END .................................C=0.01, gamma=0.01; total time=   3.4s\n",
      "[CV] END .................................C=0.01, gamma=0.01; total time=   3.4s\n",
      "[CV] END .................................C=0.01, gamma=0.01; total time=   3.4s\n",
      "[CV] END .................................C=0.01, gamma=0.01; total time=   3.5s\n",
      "[CV] END ..................................C=0.01, gamma=0.1; total time=   3.0s\n",
      "[CV] END ..................................C=0.01, gamma=0.1; total time=   3.0s\n",
      "[CV] END .................................C=0.01, gamma=0.05; total time=   3.1s\n",
      "[CV] END ..................................C=0.01, gamma=0.1; total time=   3.0s\n",
      "[CV] END .................................C=0.01, gamma=0.05; total time=   3.1s\n",
      "[CV] END ..................................C=0.01, gamma=0.1; total time=   3.1s\n",
      "[CV] END ..................................C=0.01, gamma=0.1; total time=   3.1s\n",
      "[CV] END ..................................C=0.01, gamma=0.5; total time=   3.0s\n",
      "[CV] END ..................................C=0.01, gamma=0.5; total time=   3.1s\n",
      "[CV] END ..................................C=0.01, gamma=0.5; total time=   3.1s\n",
      "[CV] END ..................................C=0.01, gamma=0.5; total time=   3.1s\n",
      "[CV] END ..................................C=0.01, gamma=0.5; total time=   3.2s\n",
      "[CV] END ....................................C=0.01, gamma=1; total time=   3.1s\n",
      "[CV] END ....................................C=0.01, gamma=1; total time=   3.1s\n",
      "[CV] END ....................................C=0.01, gamma=1; total time=   3.1s\n",
      "[CV] END ....................................C=0.01, gamma=1; total time=   3.2s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   2.9s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   3.1s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   3.1s\n",
      "[CV] END ....................................C=0.01, gamma=1; total time=   3.2s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   3.2s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   3.2s\n",
      "[CV] END ..................................C=0.1, gamma=0.05; total time=   3.2s\n",
      "[CV] END ..................................C=0.1, gamma=0.05; total time=   3.2s\n",
      "[CV] END ..................................C=0.1, gamma=0.05; total time=   3.2s\n",
      "[CV] END ..................................C=0.1, gamma=0.05; total time=   3.2s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   3.2s\n",
      "[CV] END ..................................C=0.1, gamma=0.05; total time=   3.3s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   3.2s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   3.3s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   3.2s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   3.4s\n",
      "[CV] END ...................................C=0.1, gamma=0.5; total time=   3.1s\n",
      "[CV] END ...................................C=0.1, gamma=0.5; total time=   3.1s\n",
      "[CV] END ...................................C=0.1, gamma=0.5; total time=   3.1s\n",
      "[CV] END ...................................C=0.1, gamma=0.5; total time=   3.1s\n",
      "[CV] END .....................................C=0.1, gamma=1; total time=   3.1s\n",
      "[CV] END ...................................C=0.1, gamma=0.5; total time=   3.2s\n",
      "[CV] END .....................................C=0.1, gamma=1; total time=   3.1s\n",
      "[CV] END .....................................C=0.1, gamma=1; total time=   3.1s\n",
      "[CV] END .....................................C=0.1, gamma=1; total time=   3.1s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   2.9s\n",
      "[CV] END .....................................C=0.1, gamma=1; total time=   3.1s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   3.0s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   3.0s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   3.0s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   3.1s\n",
      "[CV] END ....................................C=1, gamma=0.05; total time=   3.2s\n",
      "[CV] END ....................................C=1, gamma=0.05; total time=   3.0s\n",
      "[CV] END ....................................C=1, gamma=0.05; total time=   3.0s\n",
      "[CV] END ....................................C=1, gamma=0.05; total time=   3.1s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   3.0s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   3.0s\n",
      "[CV] END ....................................C=1, gamma=0.05; total time=   3.1s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   3.0s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   3.0s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   3.0s\n",
      "[CV] END .....................................C=1, gamma=0.5; total time=   3.0s\n",
      "[CV] END .....................................C=1, gamma=0.5; total time=   3.0s\n",
      "[CV] END .....................................C=1, gamma=0.5; total time=   3.0s\n",
      "[CV] END .....................................C=1, gamma=0.5; total time=   3.0s\n",
      "[CV] END .....................................C=1, gamma=0.5; total time=   3.0s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   3.0s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   3.1s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   2.9s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   2.9s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   3.0s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   3.0s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   3.1s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   3.1s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   3.0s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   3.1s\n",
      "[CV] END ...................................C=10, gamma=0.05; total time=   3.0s\n",
      "[CV] END ...................................C=10, gamma=0.05; total time=   3.1s\n",
      "[CV] END ...................................C=10, gamma=0.05; total time=   3.1s\n",
      "[CV] END ...................................C=10, gamma=0.05; total time=   3.1s\n",
      "[CV] END ...................................C=10, gamma=0.05; total time=   3.1s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   3.1s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   3.1s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   3.1s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   3.1s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   3.1s\n",
      "[CV] END ....................................C=10, gamma=0.5; total time=   3.0s\n",
      "[CV] END ....................................C=10, gamma=0.5; total time=   3.0s\n",
      "[CV] END ....................................C=10, gamma=0.5; total time=   3.1s\n",
      "[CV] END ....................................C=10, gamma=0.5; total time=   3.0s\n",
      "[CV] END ....................................C=10, gamma=0.5; total time=   3.1s\n",
      "[CV] END ......................................C=10, gamma=1; total time=   3.0s\n",
      "[CV] END ......................................C=10, gamma=1; total time=   3.0s\n",
      "[CV] END ......................................C=10, gamma=1; total time=   2.9s\n",
      "[CV] END ..................................C=100, gamma=0.01; total time=   2.9s\n",
      "[CV] END ......................................C=10, gamma=1; total time=   3.0s\n",
      "[CV] END ......................................C=10, gamma=1; total time=   3.0s\n",
      "[CV] END ..................................C=100, gamma=0.01; total time=   3.0s\n",
      "[CV] END ..................................C=100, gamma=0.01; total time=   2.9s\n",
      "[CV] END ..................................C=100, gamma=0.01; total time=   2.9s\n",
      "[CV] END ..................................C=100, gamma=0.01; total time=   2.8s\n",
      "[CV] END ..................................C=100, gamma=0.05; total time=   3.1s\n",
      "[CV] END ..................................C=100, gamma=0.05; total time=   3.0s\n",
      "[CV] END ..................................C=100, gamma=0.05; total time=   3.0s\n",
      "[CV] END ..................................C=100, gamma=0.05; total time=   3.1s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   3.1s\n",
      "[CV] END ..................................C=100, gamma=0.05; total time=   3.2s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   3.0s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   3.1s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   3.0s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   3.1s\n",
      "[CV] END ...................................C=100, gamma=0.5; total time=   3.2s\n",
      "[CV] END ...................................C=100, gamma=0.5; total time=   3.0s\n",
      "[CV] END ...................................C=100, gamma=0.5; total time=   3.1s\n",
      "[CV] END ...................................C=100, gamma=0.5; total time=   3.3s\n",
      "[CV] END ...................................C=100, gamma=0.5; total time=   3.1s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   2.2s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   2.4s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   2.1s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   2.1s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   2.1s\n",
      "scikit-learn implementation training time: 53.79603099822998 seconds\n",
      "Best parameters: {'C': 10, 'gamma': 0.01}\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.758\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': [0.01, 0.05, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Initialize SVM with RBF kernel\n",
    "svc = SVC(kernel='rbf')\n",
    "\n",
    "# Measure training time for scikit-learn implementation\n",
    "start_time = time.time()\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "sklearn_training_time = time.time() - start_time\n",
    "print(f\"scikit-learn implementation training time: {sklearn_training_time} seconds\")\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "best_svc = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on training and test data\n",
    "train_accuracy = best_svc.score(X_train, y_train)\n",
    "test_accuracy = best_svc.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2\n",
    "Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Original shape: (2500, 784), reduced shape: (2500, 229)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X, y = fetch_data(2500)\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Adjust the number of components as needed\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(f\"Original shape: {X.shape}, reduced shape: {X_pca.shape}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeat 2.1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with penalty value: 0.01\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.5683e+00  2.1048e+02  2e+04  3e+00  2e+04\n",
      " 1:  4.2535e+01 -2.4975e+02  3e+02  4e-02  2e+02\n",
      " 2:  2.9788e+01 -3.0577e+01  6e+01  7e-03  4e+01\n",
      " 3:  1.2327e+01 -5.2129e+00  2e+01  2e-03  9e+00\n",
      " 4:  6.1915e+00 -1.5095e+00  8e+00  6e-04  4e+00\n",
      " 5:  3.2485e+00  8.7753e-02  3e+00  2e-04  1e+00\n",
      " 6:  1.8290e+00  7.7850e-01  1e+00  6e-05  3e-01\n",
      " 7:  1.3116e+00  1.0331e+00  3e-01  6e-06  3e-02\n",
      " 8:  1.1811e+00  1.1039e+00  8e-02  8e-07  5e-03\n",
      " 9:  1.1435e+00  1.1278e+00  2e-02  1e-07  7e-04\n",
      "10:  1.1360e+00  1.1327e+00  3e-03  1e-15  9e-14\n",
      "11:  1.1343e+00  1.1342e+00  1e-04  1e-15  1e-13\n",
      "12:  1.1342e+00  1.1342e+00  6e-06  1e-15  9e-13\n",
      "13:  1.1342e+00  1.1342e+00  2e-07  1e-15  4e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.8934e+00  2.5026e+02  2e+04  3e+00  2e+04\n",
      " 1:  4.8184e+01 -3.0358e+02  4e+02  5e-02  2e+02\n",
      " 2:  3.4530e+01 -3.6286e+01  7e+01  7e-03  4e+01\n",
      " 3:  1.5713e+01 -7.2065e+00  2e+01  2e-03  1e+01\n",
      " 4:  8.4318e+00 -2.1928e+00  1e+01  8e-04  4e+00\n",
      " 5:  3.9956e+00  3.5302e-01  4e+00  2e-04  1e+00\n",
      " 6:  2.3577e+00  1.1602e+00  1e+00  6e-05  3e-01\n",
      " 7:  1.8725e+00  1.4187e+00  5e-01  1e-15  9e-15\n",
      " 8:  1.7228e+00  1.4942e+00  2e-01  8e-16  3e-14\n",
      " 9:  1.6367e+00  1.5346e+00  1e-01  8e-16  4e-14\n",
      "10:  1.5935e+00  1.5656e+00  3e-02  1e-15  8e-14\n",
      "11:  1.5809e+00  1.5749e+00  6e-03  1e-15  8e-14\n",
      "12:  1.5777e+00  1.5774e+00  3e-04  1e-15  1e-13\n",
      "13:  1.5775e+00  1.5775e+00  1e-05  1e-15  7e-13\n",
      "14:  1.5775e+00  1.5775e+00  2e-07  1e-15  3e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.9485e+00  2.4466e+02  1e+04  3e+00  1e+04\n",
      " 1:  3.9158e+01 -2.0360e+02  2e+02  4e-02  2e+02\n",
      " 2:  2.7179e+01 -2.4253e+01  5e+01  6e-03  2e+01\n",
      " 3:  1.2807e+01 -4.7309e+00  2e+01  2e-03  7e+00\n",
      " 4:  6.6406e+00 -4.9241e-01  7e+00  6e-04  2e+00\n",
      " 5:  3.2762e+00  1.3003e+00  2e+00  2e-04  6e-01\n",
      " 6:  2.3649e+00  1.7994e+00  6e-01  4e-05  1e-01\n",
      " 7:  2.1033e+00  1.9468e+00  2e-01  7e-06  3e-02\n",
      " 8:  2.0345e+00  1.9882e+00  5e-02  1e-06  5e-03\n",
      " 9:  2.0132e+00  2.0028e+00  1e-02  2e-07  9e-04\n",
      "10:  2.0076e+00  2.0068e+00  8e-04  1e-08  4e-05\n",
      "11:  2.0072e+00  2.0071e+00  3e-05  3e-10  1e-06\n",
      "12:  2.0072e+00  2.0072e+00  9e-07  7e-12  3e-08\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.2935e+00  1.8454e+02  2e+04  3e+00  2e+04\n",
      " 1:  4.2214e+01 -2.3410e+02  3e+02  4e-02  2e+02\n",
      " 2:  2.8687e+01 -3.0240e+01  6e+01  7e-03  4e+01\n",
      " 3:  1.3853e+01 -8.4011e+00  2e+01  2e-03  1e+01\n",
      " 4:  6.2300e+00 -2.4711e+00  9e+00  9e-04  5e+00\n",
      " 5:  2.9101e+00 -3.7090e-01  3e+00  3e-04  2e+00\n",
      " 6:  1.5660e+00  3.9688e-01  1e+00  7e-05  4e-01\n",
      " 7:  1.0072e+00  6.7107e-01  3e-01  5e-06  3e-02\n",
      " 8:  9.2381e-01  7.1397e-01  2e-01  2e-06  1e-02\n",
      " 9:  8.2518e-01  7.6580e-01  6e-02  3e-07  2e-03\n",
      "10:  7.9401e-01  7.8529e-01  9e-03  1e-08  9e-05\n",
      "11:  7.9011e-01  7.8818e-01  2e-03  3e-09  2e-05\n",
      "12:  7.8907e-01  7.8895e-01  1e-04  9e-12  5e-08\n",
      "13:  7.8901e-01  7.8901e-01  2e-06  2e-13  1e-09\n",
      "14:  7.8901e-01  7.8901e-01  3e-08  3e-15  2e-11\n",
      "Optimal solution found.\n",
      "Accuracy with penalty 0.01: 0.922\n",
      "Training with penalty value: 0.1\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.5208e+00  6.0291e+02  2e+04  3e+00  4e+03\n",
      " 1:  3.8238e+02 -8.1653e+02  1e+03  2e-01  2e+02\n",
      " 2:  2.0699e+02 -1.3664e+02  4e+02  4e-02  4e+01\n",
      " 3:  1.0828e+02 -5.5149e+01  2e+02  2e-02  2e+01\n",
      " 4:  6.1408e+01 -2.6883e+01  9e+01  8e-03  9e+00\n",
      " 5:  3.5051e+01 -1.1905e+01  5e+01  4e-03  4e+00\n",
      " 6:  2.0126e+01 -3.6498e+00  2e+01  2e-03  2e+00\n",
      " 7:  1.4882e+01 -8.4068e-01  2e+01  9e-04  1e+00\n",
      " 8:  6.9503e+00  3.7733e+00  3e+00  2e-04  2e-01\n",
      " 9:  5.2125e+00  4.7968e+00  4e-01  3e-06  3e-03\n",
      "10:  4.9931e+00  4.9466e+00  5e-02  4e-08  4e-05\n",
      "11:  4.9681e+00  4.9667e+00  1e-03  7e-10  9e-07\n",
      "12:  4.9673e+00  4.9673e+00  5e-05  3e-11  3e-08\n",
      "13:  4.9673e+00  4.9673e+00  1e-06  5e-13  6e-10\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.2047e+00  6.9871e+02  2e+04  4e+00  4e+03\n",
      " 1:  4.4459e+02 -9.4052e+02  1e+03  2e-01  2e+02\n",
      " 2:  2.4740e+02 -1.7553e+02  4e+02  4e-02  4e+01\n",
      " 3:  1.2562e+02 -6.3301e+01  2e+02  2e-02  2e+01\n",
      " 4:  6.3007e+01 -2.1727e+01  9e+01  7e-03  8e+00\n",
      " 5:  3.4182e+01 -4.6970e+00  4e+01  3e-03  3e+00\n",
      " 6:  2.0946e+01  2.9194e+00  2e+01  1e-03  1e+00\n",
      " 7:  1.2569e+01  7.6070e+00  5e+00  3e-04  3e-01\n",
      " 8:  1.0288e+01  8.8776e+00  1e+00  6e-05  7e-02\n",
      " 9:  9.5944e+00  9.2668e+00  3e-01  2e-15  8e-14\n",
      "10:  9.4392e+00  9.3820e+00  6e-02  2e-15  5e-13\n",
      "11:  9.4089e+00  9.4065e+00  2e-03  2e-15  6e-13\n",
      "12:  9.4077e+00  9.4075e+00  1e-04  2e-15  2e-12\n",
      "13:  9.4076e+00  9.4076e+00  3e-06  2e-15  1e-11\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.6195e+00  6.0254e+02  1e+04  3e+00  2e+03\n",
      " 1:  3.4692e+02 -6.7826e+02  1e+03  2e-01  1e+02\n",
      " 2:  1.9682e+02 -1.2817e+02  3e+02  4e-02  3e+01\n",
      " 3:  1.0057e+02 -4.0960e+01  1e+02  2e-02  1e+01\n",
      " 4:  6.0626e+01 -1.5206e+01  8e+01  7e-03  6e+00\n",
      " 5:  3.8181e+01 -1.8937e+00  4e+01  4e-03  3e+00\n",
      " 6:  2.3181e+01  6.8381e+00  2e+01  1e-03  1e+00\n",
      " 7:  1.6721e+01  1.0431e+01  6e+00  4e-04  3e-01\n",
      " 8:  1.3786e+01  1.2117e+01  2e+00  9e-05  7e-02\n",
      " 9:  1.3002e+01  1.2573e+01  4e-01  2e-05  1e-02\n",
      "10:  1.2786e+01  1.2704e+01  8e-02  2e-06  2e-03\n",
      "11:  1.2742e+01  1.2735e+01  7e-03  1e-07  1e-04\n",
      "12:  1.2738e+01  1.2738e+01  1e-04  2e-09  2e-06\n",
      "13:  1.2738e+01  1.2738e+01  2e-06  5e-11  4e-08\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.3570e+00  5.7381e+02  2e+04  3e+00  4e+03\n",
      " 1:  3.8191e+02 -6.7458e+02  1e+03  1e-01  2e+02\n",
      " 2:  1.9363e+02 -1.1917e+02  3e+02  3e-02  4e+01\n",
      " 3:  9.4022e+01 -4.6216e+01  1e+02  1e-02  1e+01\n",
      " 4:  5.6667e+01 -2.4976e+01  8e+01  6e-03  8e+00\n",
      " 5:  3.2145e+01 -1.1789e+01  4e+01  3e-03  4e+00\n",
      " 6:  2.1635e+01 -6.5684e+00  3e+01  2e-03  2e+00\n",
      " 7:  1.2340e+01 -1.6210e+00  1e+01  7e-04  9e-01\n",
      " 8:  5.8480e+00  1.7044e+00  4e+00  2e-04  2e-01\n",
      " 9:  3.7250e+00  2.7923e+00  9e-01  2e-05  3e-02\n",
      "10:  3.1975e+00  3.0841e+00  1e-01  2e-15  6e-14\n",
      "11:  3.1410e+00  3.1279e+00  1e-02  2e-15  4e-13\n",
      "12:  3.1339e+00  3.1335e+00  4e-04  2e-15  5e-13\n",
      "13:  3.1337e+00  3.1337e+00  1e-05  3e-15  5e-12\n",
      "14:  3.1337e+00  3.1337e+00  2e-07  2e-15  2e-11\n",
      "Optimal solution found.\n",
      "Accuracy with penalty 0.1: 0.91\n",
      "Training with penalty value: 1\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.6654e+03  4.7424e+03  3e+04  5e+00  4e+02\n",
      " 1:  1.9847e+03 -2.0434e+03  6e+03  6e-01  6e+01\n",
      " 2:  1.0073e+03 -7.7101e+02  2e+03  2e-01  2e+01\n",
      " 3:  5.4219e+02 -3.7742e+02  1e+03  1e-01  1e+01\n",
      " 4:  3.6188e+02 -2.2910e+02  7e+02  6e-02  5e+00\n",
      " 5:  2.7901e+02 -1.6681e+02  5e+02  3e-02  3e+00\n",
      " 6:  1.9759e+02 -1.0695e+02  3e+02  2e-02  2e+00\n",
      " 7:  1.4768e+02 -7.0461e+01  2e+02  1e-02  9e-01\n",
      " 8:  8.7222e+01 -2.5516e+01  1e+02  4e-03  4e-01\n",
      " 9:  5.6016e+01 -5.1588e+00  6e+01  2e-03  1e-01\n",
      "10:  3.9984e+01  4.1731e+00  4e+01  9e-04  8e-02\n",
      "11:  3.6180e+01  6.2009e+00  3e+01  6e-04  6e-02\n",
      "12:  2.8860e+01  9.9294e+00  2e+01  9e-05  8e-03\n",
      "13:  2.1997e+01  1.3935e+01  8e+00  3e-05  3e-03\n",
      "14:  1.8443e+01  1.5819e+01  3e+00  6e-07  5e-05\n",
      "15:  1.7309e+01  1.6579e+01  7e-01  4e-15  3e-13\n",
      "16:  1.6924e+01  1.6880e+01  4e-02  4e-15  5e-13\n",
      "17:  1.6900e+01  1.6900e+01  8e-04  4e-15  1e-12\n",
      "18:  1.6900e+01  1.6900e+01  2e-05  4e-15  6e-12\n",
      "19:  1.6900e+01  1.6900e+01  4e-07  6e-15  6e-11\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.5974e+03  5.8106e+03  4e+04  5e+00  4e+02\n",
      " 1:  2.6945e+03 -3.2913e+03  8e+03  8e-01  7e+01\n",
      " 2:  1.5598e+03 -1.2877e+03  3e+03  3e-01  3e+01\n",
      " 3:  6.7561e+02 -3.9506e+02  1e+03  1e-01  9e+00\n",
      " 4:  3.7231e+02 -1.5596e+02  6e+02  5e-02  4e+00\n",
      " 5:  2.5460e+02 -7.0216e+01  4e+02  2e-02  2e+00\n",
      " 6:  1.8354e+02 -2.2471e+01  2e+02  1e-02  1e+00\n",
      " 7:  1.5769e+02 -7.5400e+00  2e+02  8e-03  7e-01\n",
      " 8:  1.0583e+02  2.8093e+01  8e+01  3e-03  3e-01\n",
      " 9:  7.0015e+01  5.2208e+01  2e+01  5e-04  4e-02\n",
      "10:  6.1204e+01  5.7746e+01  4e+00  6e-05  5e-03\n",
      "11:  5.9367e+01  5.8932e+01  4e-01  5e-07  5e-05\n",
      "12:  5.9131e+01  5.9118e+01  1e-02  1e-08  1e-06\n",
      "13:  5.9124e+01  5.9124e+01  2e-04  2e-10  2e-08\n",
      "14:  5.9124e+01  5.9124e+01  3e-06  2e-12  2e-10\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.5902e+03  4.6340e+03  3e+04  4e+00  3e+02\n",
      " 1:  1.8940e+03 -1.9513e+03  5e+03  7e-01  4e+01\n",
      " 2:  1.1043e+03 -7.3896e+02  2e+03  2e-01  1e+01\n",
      " 3:  6.2205e+02 -3.1117e+02  1e+03  1e-01  7e+00\n",
      " 4:  3.9166e+02 -1.2417e+02  6e+02  5e-02  3e+00\n",
      " 5:  3.3389e+02 -9.0434e+01  5e+02  3e-02  2e+00\n",
      " 6:  2.3112e+02 -2.3342e+01  3e+02  2e-02  1e+00\n",
      " 7:  1.7962e+02  8.7101e+00  2e+02  1e-02  6e-01\n",
      " 8:  1.2931e+02  4.3550e+01  9e+01  4e-03  3e-01\n",
      " 9:  1.0224e+02  6.2525e+01  4e+01  2e-03  9e-02\n",
      "10:  8.4213e+01  7.4522e+01  1e+01  3e-04  2e-02\n",
      "11:  7.9291e+01  7.7742e+01  2e+00  4e-06  2e-04\n",
      "12:  7.8625e+01  7.8234e+01  4e-01  5e-07  3e-05\n",
      "13:  7.8419e+01  7.8397e+01  2e-02  9e-09  6e-07\n",
      "14:  7.8407e+01  7.8406e+01  8e-04  3e-10  2e-08\n",
      "15:  7.8407e+01  7.8407e+01  8e-06  3e-12  2e-10\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.7167e+03  4.6139e+03  3e+04  4e+00  4e+02\n",
      " 1:  1.7507e+03 -2.2360e+03  6e+03  7e-01  7e+01\n",
      " 2:  8.2608e+02 -7.5366e+02  2e+03  2e-01  2e+01\n",
      " 3:  4.9588e+02 -4.1812e+02  1e+03  1e-01  1e+01\n",
      " 4:  3.2096e+02 -2.4805e+02  7e+02  7e-02  6e+00\n",
      " 5:  2.4189e+02 -1.9047e+02  5e+02  4e-02  4e+00\n",
      " 6:  2.1640e+02 -1.7235e+02  4e+02  3e-02  3e+00\n",
      " 7:  1.7893e+02 -1.3012e+02  3e+02  2e-02  2e+00\n",
      " 8:  9.1222e+01 -6.0210e+01  2e+02  9e-03  8e-01\n",
      " 9:  8.7139e+01 -5.3630e+01  1e+02  6e-03  6e-01\n",
      "10:  3.6177e+01 -1.3461e+01  5e+01  2e-03  1e-01\n",
      "11:  1.2629e+01  1.2782e+00  1e+01  4e-15  9e-15\n",
      "12:  8.3969e+00  2.9744e+00  5e+00  3e-15  5e-14\n",
      "13:  6.4778e+00  3.8003e+00  3e+00  3e-15  5e-14\n",
      "14:  5.7052e+00  4.1583e+00  2e+00  2e-15  4e-14\n",
      "15:  4.9509e+00  4.5922e+00  4e-01  4e-15  8e-14\n",
      "16:  4.7511e+00  4.7101e+00  4e-02  4e-15  1e-13\n",
      "17:  4.7281e+00  4.7270e+00  1e-03  3e-15  1e-13\n",
      "18:  4.7275e+00  4.7275e+00  2e-05  4e-15  1e-12\n",
      "19:  4.7275e+00  4.7275e+00  3e-07  4e-15  3e-12\n",
      "Optimal solution found.\n",
      "Accuracy with penalty 1: 0.896\n",
      "Training with penalty value: 10\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.8161e+05  1.5429e+05  6e+05  2e+01  1e+02\n",
      " 1:  5.8472e+03 -3.8278e+04  1e+05  4e+00  2e+01\n",
      " 2:  5.5339e+03 -1.5791e+04  4e+04  1e+00  8e+00\n",
      " 3:  5.6257e+03 -9.1333e+03  2e+04  7e-01  4e+00\n",
      " 4:  4.1813e+03 -4.8248e+03  1e+04  3e-01  2e+00\n",
      " 5:  3.6930e+03 -3.8406e+03  1e+04  2e-01  1e+00\n",
      " 6:  3.3395e+03 -3.3188e+03  9e+03  1e-01  8e-01\n",
      " 7:  2.9643e+03 -1.9981e+03  6e+03  6e-02  3e-01\n",
      " 8:  6.8629e+02 -3.6414e+02  1e+03  1e-02  6e-02\n",
      " 9:  9.3139e+01 -2.8267e+01  1e+02  5e-05  3e-04\n",
      "10:  5.8206e+01 -7.5247e+00  7e+01  2e-05  1e-04\n",
      "11:  5.2853e+01  1.7957e-01  5e+01  1e-05  6e-05\n",
      "12:  4.0324e+01  8.1835e+00  3e+01  3e-06  2e-05\n",
      "13:  2.9841e+01  1.3748e+01  2e+01  1e-06  7e-06\n",
      "14:  2.3725e+01  1.6839e+01  7e+00  5e-15  3e-14\n",
      "15:  2.0088e+01  1.9198e+01  9e-01  7e-15  4e-14\n",
      "16:  1.9631e+01  1.9527e+01  1e-01  5e-15  8e-14\n",
      "17:  1.9573e+01  1.9570e+01  3e-03  7e-15  2e-13\n",
      "18:  1.9571e+01  1.9571e+01  5e-05  6e-15  5e-13\n",
      "19:  1.9571e+01  1.9571e+01  6e-07  6e-15  6e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.7786e+05  1.4182e+05  5e+05  2e+01  1e+02\n",
      " 1: -8.3560e+03 -3.8900e+04  1e+05  6e+00  3e+01\n",
      " 2:  3.0268e+03 -1.6691e+04  5e+04  2e+00  8e+00\n",
      " 3:  2.8846e+03 -7.2108e+03  2e+04  7e-01  3e+00\n",
      " 4:  2.4847e+03 -3.5674e+03  1e+04  3e-01  1e+00\n",
      " 5:  2.8590e+03 -2.3878e+03  7e+03  2e-01  7e-01\n",
      " 6:  2.1027e+03 -1.3908e+03  5e+03  8e-02  4e-01\n",
      " 7:  1.6032e+03 -6.7509e+02  3e+03  4e-02  2e-01\n",
      " 8:  8.1975e+02 -3.9812e+01  1e+03  8e-03  4e-02\n",
      " 9:  4.4857e+02  1.6012e+02  3e+02  1e-03  7e-03\n",
      "10:  3.5119e+02  2.1451e+02  1e+02  4e-04  2e-03\n",
      "11:  3.0810e+02  2.3696e+02  7e+01  1e-04  5e-04\n",
      "12:  2.7675e+02  2.5552e+02  2e+01  2e-05  1e-04\n",
      "13:  2.6617e+02  2.6208e+02  4e+00  3e-06  1e-05\n",
      "14:  2.6404e+02  2.6350e+02  5e-01  3e-07  1e-06\n",
      "15:  2.6373e+02  2.6373e+02  8e-03  3e-09  1e-08\n",
      "16:  2.6373e+02  2.6373e+02  9e-05  3e-11  2e-10\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.7747e+05  1.4461e+05  5e+05  2e+01  9e+01\n",
      " 1: -6.1852e+03 -3.8877e+04  1e+05  6e+00  2e+01\n",
      " 2:  3.7245e+03 -1.9045e+04  5e+04  2e+00  7e+00\n",
      " 3:  3.9297e+03 -8.3291e+03  2e+04  8e-01  3e+00\n",
      " 4:  4.8731e+03 -5.2089e+03  2e+04  4e-01  1e+00\n",
      " 5:  3.3769e+03 -2.4967e+03  8e+03  2e-01  7e-01\n",
      " 6:  2.4029e+03 -8.4096e+02  4e+03  6e-02  2e-01\n",
      " 7:  1.2951e+03 -3.0923e+01  2e+03  2e-02  7e-02\n",
      " 8:  8.8599e+02  2.2760e+02  7e+02  6e-03  2e-02\n",
      " 9:  6.3433e+02  3.6093e+02  3e+02  6e-04  2e-03\n",
      "10:  5.7866e+02  3.8572e+02  2e+02  3e-04  1e-03\n",
      "11:  5.0151e+02  4.2179e+02  8e+01  1e-04  4e-04\n",
      "12:  4.6448e+02  4.3852e+02  3e+01  1e-14  5e-13\n",
      "13:  4.5367e+02  4.4561e+02  8e+00  9e-15  9e-13\n",
      "14:  4.4962e+02  4.4857e+02  1e+00  1e-14  2e-12\n",
      "15:  4.4905e+02  4.4903e+02  2e-02  1e-14  3e-12\n",
      "16:  4.4904e+02  4.4904e+02  2e-04  2e-14  2e-11\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.8443e+05  1.2330e+05  4e+05  2e+01  1e+02\n",
      " 1: -6.8423e+03 -2.7733e+04  9e+04  4e+00  2e+01\n",
      " 2: -6.8851e+02 -1.4560e+04  4e+04  2e+00  8e+00\n",
      " 3:  2.4920e+02 -9.7969e+03  2e+04  1e+00  5e+00\n",
      " 4:  3.2310e+03 -5.6440e+03  1e+04  4e-01  2e+00\n",
      " 5:  3.7716e+03 -3.5283e+03  1e+04  2e-01  9e-01\n",
      " 6:  9.1556e+02 -6.9867e+02  2e+03  3e-02  2e-01\n",
      " 7:  2.8563e+02 -1.6097e+02  5e+02  7e-03  4e-02\n",
      " 8:  2.8856e+01 -1.3503e+01  5e+01  3e-04  1e-03\n",
      " 9:  1.3484e+01 -2.2830e+00  2e+01  8e-05  4e-04\n",
      "10:  1.0891e+01  4.6916e-01  1e+01  3e-05  2e-04\n",
      "11:  9.5235e+00  2.3102e+00  7e+00  5e-06  3e-05\n",
      "12:  7.1096e+00  3.4430e+00  4e+00  2e-06  8e-06\n",
      "13:  5.6293e+00  4.1881e+00  1e+00  3e-07  1e-06\n",
      "14:  4.9278e+00  4.5860e+00  3e-01  2e-08  1e-07\n",
      "15:  4.7456e+00  4.7125e+00  3e-02  4e-10  2e-09\n",
      "16:  4.7279e+00  4.7271e+00  7e-04  8e-12  4e-11\n",
      "17:  4.7275e+00  4.7275e+00  1e-05  1e-13  6e-13\n",
      "18:  4.7275e+00  4.7275e+00  1e-07  3e-15  1e-12\n",
      "Optimal solution found.\n",
      "Accuracy with penalty 10: 0.878\n",
      "Training with penalty value: 100\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.8311e+07  1.2379e+07  4e+07  2e+02  1e+02\n",
      " 1: -7.7114e+05 -2.8854e+06  9e+06  4e+01  2e+01\n",
      " 2:  3.6428e+04 -1.3021e+06  3e+06  1e+01  7e+00\n",
      " 3:  2.6166e+05 -7.8348e+05  2e+06  7e+00  3e+00\n",
      " 4:  2.3968e+05 -5.1200e+05  1e+06  4e+00  2e+00\n",
      " 5:  2.8369e+05 -4.1414e+05  1e+06  2e+00  1e+00\n",
      " 6:  3.5459e+05 -2.9486e+05  8e+05  1e+00  6e-01\n",
      " 7:  7.7383e+04 -5.6383e+04  2e+05  2e-01  1e-01\n",
      " 8:  2.2071e+04 -1.1879e+04  4e+04  4e-02  2e-02\n",
      " 9:  1.8476e+03 -1.3341e+03  3e+03  8e-04  4e-04\n",
      "10:  2.3952e+02 -1.7762e+02  4e+02  7e-06  3e-06\n",
      "11:  8.6487e+01 -4.7456e+01  1e+02  2e-06  1e-06\n",
      "12:  4.9481e+01 -1.1255e+01  6e+01  6e-07  3e-07\n",
      "13:  4.4006e+01 -1.1883e+00  5e+01  3e-07  2e-07\n",
      "14:  3.9345e+01  7.5516e+00  3e+01  9e-08  4e-08\n",
      "15:  2.9580e+01  1.3422e+01  2e+01  3e-08  2e-08\n",
      "16:  2.4086e+01  1.6657e+01  7e+00  5e-15  2e-15\n",
      "17:  2.0066e+01  1.9207e+01  9e-01  7e-15  6e-15\n",
      "18:  1.9631e+01  1.9526e+01  1e-01  5e-15  7e-15\n",
      "19:  1.9573e+01  1.9570e+01  3e-03  7e-15  4e-14\n",
      "20:  1.9571e+01  1.9571e+01  5e-05  6e-15  5e-14\n",
      "21:  1.9571e+01  1.9571e+01  7e-07  6e-15  1e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.7967e+07  1.1203e+07  4e+07  2e+02  9e+01\n",
      " 1: -2.6158e+06 -3.0391e+06  1e+07  6e+01  2e+01\n",
      " 2: -3.9340e+05 -1.2909e+06  3e+06  2e+01  7e+00\n",
      " 3: -4.2563e+04 -5.8090e+05  1e+06  6e+00  3e+00\n",
      " 4:  1.5737e+05 -3.5031e+05  9e+05  3e+00  1e+00\n",
      " 5:  1.6156e+05 -2.3990e+05  6e+05  2e+00  7e-01\n",
      " 6:  1.2658e+05 -1.3140e+05  4e+05  8e-01  3e-01\n",
      " 7:  1.0321e+05 -8.5292e+04  3e+05  5e-01  2e-01\n",
      " 8:  1.1129e+05 -5.6688e+04  2e+05  2e-01  8e-02\n",
      " 9:  1.5688e+04 -7.3374e+03  2e+04  1e-02  5e-03\n",
      "10:  2.8885e+03 -1.2237e+03  4e+03  1e-03  6e-04\n",
      "11:  1.6650e+03 -4.0327e+02  2e+03  5e-04  2e-04\n",
      "12:  1.1750e+03 -1.0056e+02  1e+03  2e-04  7e-05\n",
      "13:  9.2332e+02  8.7857e+01  8e+02  3e-05  1e-05\n",
      "14:  6.1341e+02  2.1486e+02  4e+02  9e-06  4e-06\n",
      "15:  5.0113e+02  2.6543e+02  2e+02  3e-06  1e-06\n",
      "16:  3.9930e+02  3.1556e+02  8e+01  4e-07  1e-07\n",
      "17:  3.6156e+02  3.3806e+02  2e+01  6e-08  2e-08\n",
      "18:  3.5136e+02  3.4484e+02  7e+00  1e-08  6e-09\n",
      "19:  3.4771e+02  3.4730e+02  4e-01  5e-10  2e-10\n",
      "20:  3.4748e+02  3.4747e+02  8e-03  9e-12  4e-12\n",
      "21:  3.4748e+02  3.4748e+02  3e-04  3e-13  4e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.7931e+07  1.1455e+07  4e+07  2e+02  7e+01\n",
      " 1: -1.4323e+06 -2.7805e+06  1e+07  5e+01  2e+01\n",
      " 2: -1.4769e+05 -1.3786e+06  4e+06  2e+01  5e+00\n",
      " 3:  6.5358e+04 -6.5760e+05  2e+06  7e+00  2e+00\n",
      " 4:  1.8835e+05 -3.7093e+05  1e+06  3e+00  1e+00\n",
      " 5:  1.3609e+05 -1.4935e+05  5e+05  1e+00  4e-01\n",
      " 6:  1.0193e+05 -6.3690e+04  2e+05  5e-01  1e-01\n",
      " 7:  6.8992e+04 -2.5984e+04  1e+05  1e-01  4e-02\n",
      " 8:  3.1438e+04 -6.9125e+03  4e+04  1e-02  4e-03\n",
      " 9:  3.0066e+04 -6.0195e+03  4e+04  8e-03  3e-03\n",
      "10:  1.0339e+04 -1.4153e+03  1e+04  2e-03  7e-04\n",
      "11:  6.3538e+03 -2.3575e+02  7e+03  5e-04  2e-04\n",
      "12:  4.2705e+03  3.8765e+02  4e+03  2e-04  7e-05\n",
      "13:  3.0912e+03  7.9483e+02  2e+03  9e-05  3e-05\n",
      "14:  2.4623e+03  1.0495e+03  1e+03  3e-05  1e-05\n",
      "15:  1.8273e+03  1.3274e+03  5e+02  7e-06  2e-06\n",
      "16:  1.6158e+03  1.4311e+03  2e+02  2e-06  5e-07\n",
      "17:  1.5477e+03  1.4696e+03  8e+01  5e-07  2e-07\n",
      "18:  1.5175e+03  1.4858e+03  3e+01  3e-14  2e-12\n",
      "19:  1.5011e+03  1.4988e+03  2e+00  4e-14  2e-12\n",
      "20:  1.4998e+03  1.4998e+03  2e-02  4e-14  5e-12\n",
      "21:  1.4998e+03  1.4998e+03  2e-04  4e-14  2e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.8570e+07  9.5565e+06  3e+07  2e+02  9e+01\n",
      " 1: -1.7104e+06 -2.0838e+06  7e+06  4e+01  2e+01\n",
      " 2: -4.8870e+05 -9.8792e+05  3e+06  1e+01  6e+00\n",
      " 3: -2.0260e+05 -7.9669e+05  2e+06  9e+00  4e+00\n",
      " 4: -1.3701e+04 -5.7811e+05  1e+06  6e+00  2e+00\n",
      " 5:  2.9161e+05 -2.0538e+05  7e+05  1e+00  6e-01\n",
      " 6:  7.7907e+04 -4.2096e+04  2e+05  2e-01  1e-01\n",
      " 7:  1.4659e+04 -6.1657e+03  2e+04  2e-02  1e-02\n",
      " 8:  9.0876e+02 -6.9029e+02  2e+03  4e-04  2e-04\n",
      " 9:  1.1647e+02 -9.1659e+01  2e+02  7e-08  3e-08\n",
      "10:  4.8024e+01 -2.8761e+01  8e+01  2e-08  9e-09\n",
      "11:  2.2901e+01 -7.1014e+00  3e+01  5e-09  2e-09\n",
      "12:  1.3019e+01 -1.0643e+00  1e+01  2e-09  7e-10\n",
      "13:  1.0274e+01  9.5912e-01  9e+00  8e-10  3e-10\n",
      "14:  8.4352e+00  2.6302e+00  6e+00  2e-10  9e-11\n",
      "15:  6.6293e+00  3.6442e+00  3e+00  5e-11  2e-11\n",
      "16:  5.4414e+00  4.2829e+00  1e+00  4e-12  2e-12\n",
      "17:  4.8875e+00  4.6245e+00  3e-01  9e-13  4e-13\n",
      "18:  4.7450e+00  4.7151e+00  3e-02  7e-14  3e-14\n",
      "19:  4.7279e+00  4.7272e+00  7e-04  4e-15  4e-15\n",
      "20:  4.7275e+00  4.7275e+00  1e-05  3e-15  6e-15\n",
      "21:  4.7275e+00  4.7275e+00  1e-07  3e-15  5e-14\n",
      "Optimal solution found.\n",
      "Accuracy with penalty 100: 0.864\n"
     ]
    }
   ],
   "source": [
    "# Define a range of slack values to test\n",
    "penalties = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "for penalty in penalties:\n",
    "    print(f\"Training with penalty value: {penalty}\")\n",
    "    # Train one-vs-all SVM\n",
    "    classifiers = train_one_vs_all_svm_1a(X_train, y_train, class_labels, C=penalty)\n",
    "\n",
    "    # Predict using one-vs-all SVM\n",
    "    y_pred = predict_one_vs_all_svm_1a(X_test, classifiers, class_labels)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(f\"Accuracy with penalty {penalty}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repeat 2.1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "[CV] END .............................................C=0.01; total time=   0.1s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END ..............................................C=0.1; total time=   0.5s\n",
      "[CV] END ..............................................C=0.1; total time=   0.5s\n",
      "[CV] END ..............................................C=0.1; total time=   0.6s\n",
      "[CV] END ..............................................C=0.1; total time=   0.5s\n",
      "[CV] END ..............................................C=0.1; total time=   0.5s\n",
      "[CV] END ................................................C=1; total time=   1.6s\n",
      "[CV] END ................................................C=1; total time=   2.0s\n",
      "[CV] END ................................................C=1; total time=   2.0s\n",
      "[CV] END ................................................C=1; total time=   1.7s\n",
      "[CV] END ................................................C=1; total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...............................................C=10; total time=   3.4s\n",
      "[CV] END ...............................................C=10; total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...............................................C=10; total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...............................................C=10; total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...............................................C=10; total time=   3.4s\n",
      "[CV] END ..............................................C=100; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ..............................................C=100; total time=   3.6s\n",
      "[CV] END ..............................................C=100; total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ..............................................C=100; total time=   3.0s\n",
      "[CV] END ..............................................C=100; total time=   3.1s\n",
      "Best parameters: {'C': 0.01}\n",
      "C = 0.01 | Training Accuracy: 0.973 | Test Accuracy: 0.902\n",
      "C = 0.1 | Training Accuracy: 0.987 | Test Accuracy: 0.888\n",
      "C = 1 | Training Accuracy: 0.995 | Test Accuracy: 0.882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 10 | Training Accuracy: 0.988 | Test Accuracy: 0.858\n",
      "C = 100 | Training Accuracy: 0.982 | Test Accuracy: 0.842\n",
      "Training Time: 10.49 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "b1(X_pca, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repeat 2.1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with penalty value: 10, gamma value: 0.01\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  5.8421e+03 -9.1194e+04  2e+05  3e-01  7e-15\n",
      " 1:  3.3563e+03 -1.0029e+04  1e+04  7e-03  3e-15\n",
      " 2:  9.7709e+01 -2.4997e+03  3e+03  6e-04  4e-15\n",
      " 3: -4.2847e+02 -7.1028e+02  3e+02  6e-14  2e-15\n",
      " 4: -4.6293e+02 -5.0510e+02  4e+01  3e-14  7e-16\n",
      " 5: -4.6738e+02 -4.7647e+02  9e+00  6e-15  4e-16\n",
      " 6: -4.6858e+02 -4.7019e+02  2e+00  8e-15  3e-16\n",
      " 7: -4.6887e+02 -4.6907e+02  2e-01  5e-15  3e-16\n",
      " 8: -4.6891e+02 -4.6892e+02  1e-02  3e-14  3e-16\n",
      " 9: -4.6891e+02 -4.6891e+02  4e-04  2e-14  3e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  5.6598e+03 -8.5211e+04  1e+05  3e-01  8e-15\n",
      " 1:  3.3505e+03 -1.0352e+04  1e+04  9e-03  3e-15\n",
      " 2:  2.7203e+02 -2.7094e+03  3e+03  1e-03  4e-15\n",
      " 3: -3.3771e+02 -6.8082e+02  3e+02  2e-14  2e-15\n",
      " 4: -3.8068e+02 -4.4041e+02  6e+01  2e-14  7e-16\n",
      " 5: -3.8614e+02 -3.9800e+02  1e+01  5e-15  4e-16\n",
      " 6: -3.8733e+02 -3.8970e+02  2e+00  2e-14  3e-16\n",
      " 7: -3.8762e+02 -3.8813e+02  5e-01  1e-14  3e-16\n",
      " 8: -3.8770e+02 -3.8775e+02  5e-02  2e-14  3e-16\n",
      " 9: -3.8771e+02 -3.8772e+02  2e-03  1e-15  3e-16\n",
      "10: -3.8772e+02 -3.8772e+02  7e-05  1e-14  3e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  5.4414e+03 -7.8437e+04  1e+05  3e-01  8e-15\n",
      " 1:  3.1238e+03 -1.0323e+04  1e+04  1e-02  3e-15\n",
      " 2:  9.8831e+01 -1.9200e+03  2e+03  2e-04  4e-15\n",
      " 3: -3.2791e+02 -5.4267e+02  2e+02  1e-13  2e-15\n",
      " 4: -3.5373e+02 -3.8711e+02  3e+01  6e-15  6e-16\n",
      " 5: -3.5715e+02 -3.6337e+02  6e+00  3e-15  4e-16\n",
      " 6: -3.5792e+02 -3.5911e+02  1e+00  6e-15  3e-16\n",
      " 7: -3.5810e+02 -3.5821e+02  1e-01  2e-15  3e-16\n",
      " 8: -3.5812e+02 -3.5813e+02  8e-03  4e-15  3e-16\n",
      " 9: -3.5812e+02 -3.5813e+02  2e-04  4e-16  4e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  5.4992e+03 -8.2903e+04  2e+05  3e-01  8e-15\n",
      " 1:  3.5159e+03 -1.0446e+04  1e+04  8e-03  4e-15\n",
      " 2:  4.3316e+02 -2.8054e+03  3e+03  1e-03  4e-15\n",
      " 3: -2.4845e+02 -6.3689e+02  4e+02  1e-13  2e-15\n",
      " 4: -3.0371e+02 -3.6014e+02  6e+01  4e-14  8e-16\n",
      " 5: -3.0972e+02 -3.2357e+02  1e+01  2e-14  4e-16\n",
      " 6: -3.1130e+02 -3.1366e+02  2e+00  2e-16  3e-16\n",
      " 7: -3.1167e+02 -3.1204e+02  4e-01  1e-14  4e-16\n",
      " 8: -3.1175e+02 -3.1177e+02  2e-02  2e-15  4e-16\n",
      " 9: -3.1175e+02 -3.1175e+02  8e-04  2e-14  4e-16\n",
      "10: -3.1175e+02 -3.1175e+02  3e-05  5e-15  4e-16\n",
      "Optimal solution found.\n",
      "Accuracy with penalty 10 and gamma 0.01: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Define a range of slack values to test\n",
    "penalties = [10]\n",
    "gamma_values = [0.01]\n",
    "\n",
    "# Train and evaluate for each combination of penalty and gamma value\n",
    "for penalty in penalties:\n",
    "    for gamma in gamma_values:\n",
    "        print(f\"Training with penalty value: {penalty}, gamma value: {gamma}\")\n",
    "        classifiers = train_one_vs_all_svm_1c(X_train, y_train, class_labels, C=penalty, gamma=gamma)\n",
    "        y_pred = predict_one_vs_all_svm_1c(X_test, classifiers, class_labels, gamma)\n",
    "\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        print(f\"Accuracy with penalty {penalty} and gamma {gamma}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repeat 2.1.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 229)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[CV] END .................................C=0.01, gamma=0.01; total time=   0.6s\n",
      "[CV] END .................................C=0.01, gamma=0.01; total time=   0.7s\n",
      "[CV] END .................................C=0.01, gamma=0.01; total time=   0.7s\n",
      "[CV] END .................................C=0.01, gamma=0.01; total time=   0.8s\n",
      "[CV] END .................................C=0.01, gamma=0.01; total time=   0.8s\n",
      "[CV] END .................................C=0.01, gamma=0.05; total time=   0.7s\n",
      "[CV] END .................................C=0.01, gamma=0.05; total time=   0.8s\n",
      "[CV] END .................................C=0.01, gamma=0.05; total time=   0.8s\n",
      "[CV] END .................................C=0.01, gamma=0.05; total time=   0.7s\n",
      "[CV] END .................................C=0.01, gamma=0.05; total time=   0.7s\n",
      "[CV] END ..................................C=0.01, gamma=0.1; total time=   0.7s\n",
      "[CV] END ..................................C=0.01, gamma=0.1; total time=   0.7s\n",
      "[CV] END ..................................C=0.01, gamma=0.1; total time=   0.7s\n",
      "[CV] END ..................................C=0.01, gamma=0.1; total time=   0.7s\n",
      "[CV] END ..................................C=0.01, gamma=0.1; total time=   0.7s\n",
      "[CV] END ..................................C=0.01, gamma=0.5; total time=   0.7s\n",
      "[CV] END ..................................C=0.01, gamma=0.5; total time=   0.6s\n",
      "[CV] END ..................................C=0.01, gamma=0.5; total time=   0.7s\n",
      "[CV] END ..................................C=0.01, gamma=0.5; total time=   0.7s\n",
      "[CV] END ..................................C=0.01, gamma=0.5; total time=   0.7s\n",
      "[CV] END ....................................C=0.01, gamma=1; total time=   0.7s\n",
      "[CV] END ....................................C=0.01, gamma=1; total time=   0.7s\n",
      "[CV] END ....................................C=0.01, gamma=1; total time=   0.7s\n",
      "[CV] END ....................................C=0.01, gamma=1; total time=   0.7s\n",
      "[CV] END ....................................C=0.01, gamma=1; total time=   0.6s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   0.6s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   0.5s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   0.6s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   0.6s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   0.7s\n",
      "[CV] END ..................................C=0.1, gamma=0.05; total time=   0.6s\n",
      "[CV] END ..................................C=0.1, gamma=0.05; total time=   0.7s\n",
      "[CV] END ..................................C=0.1, gamma=0.05; total time=   0.6s\n",
      "[CV] END ..................................C=0.1, gamma=0.05; total time=   0.6s\n",
      "[CV] END ..................................C=0.1, gamma=0.05; total time=   0.7s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   0.7s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   0.7s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   0.6s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   0.7s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   0.7s\n",
      "[CV] END ...................................C=0.1, gamma=0.5; total time=   0.7s\n",
      "[CV] END ...................................C=0.1, gamma=0.5; total time=   0.7s\n",
      "[CV] END ...................................C=0.1, gamma=0.5; total time=   0.7s\n",
      "[CV] END ...................................C=0.1, gamma=0.5; total time=   0.7s\n",
      "[CV] END ...................................C=0.1, gamma=0.5; total time=   0.7s\n",
      "[CV] END .....................................C=0.1, gamma=1; total time=   0.7s\n",
      "[CV] END .....................................C=0.1, gamma=1; total time=   0.8s\n",
      "[CV] END .....................................C=0.1, gamma=1; total time=   0.6s\n",
      "[CV] END .....................................C=0.1, gamma=1; total time=   0.7s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   0.6s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   0.6s\n",
      "[CV] END .....................................C=0.1, gamma=1; total time=   0.7s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   0.6s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   0.7s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   0.6s\n",
      "[CV] END ....................................C=1, gamma=0.05; total time=   0.7s\n",
      "[CV] END ....................................C=1, gamma=0.05; total time=   0.6s\n",
      "[CV] END ....................................C=1, gamma=0.05; total time=   0.6s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   0.6s\n",
      "[CV] END ....................................C=1, gamma=0.05; total time=   0.7s\n",
      "[CV] END ....................................C=1, gamma=0.05; total time=   0.7s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   0.7s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   0.7s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   0.6s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   0.6s\n",
      "[CV] END .....................................C=1, gamma=0.5; total time=   0.7s\n",
      "[CV] END .....................................C=1, gamma=0.5; total time=   0.7s\n",
      "[CV] END .....................................C=1, gamma=0.5; total time=   0.6s\n",
      "[CV] END .....................................C=1, gamma=0.5; total time=   0.7s\n",
      "[CV] END .....................................C=1, gamma=0.5; total time=   0.6s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   0.7s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   0.7s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   0.7s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   0.6s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   0.6s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   0.7s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   0.7s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   0.6s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   0.6s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   0.6s\n",
      "[CV] END ...................................C=10, gamma=0.05; total time=   0.7s\n",
      "[CV] END ...................................C=10, gamma=0.05; total time=   0.7s\n",
      "[CV] END ...................................C=10, gamma=0.05; total time=   0.6s\n",
      "[CV] END ...................................C=10, gamma=0.05; total time=   0.7s\n",
      "[CV] END ...................................C=10, gamma=0.05; total time=   0.7s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   0.7s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   0.6s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   0.7s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   0.7s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   0.7s\n",
      "[CV] END ....................................C=10, gamma=0.5; total time=   0.7s\n",
      "[CV] END ....................................C=10, gamma=0.5; total time=   0.7s\n",
      "[CV] END ....................................C=10, gamma=0.5; total time=   0.7s\n",
      "[CV] END ....................................C=10, gamma=0.5; total time=   0.7s\n",
      "[CV] END ....................................C=10, gamma=0.5; total time=   0.7s\n",
      "[CV] END ......................................C=10, gamma=1; total time=   0.7s\n",
      "[CV] END ......................................C=10, gamma=1; total time=   0.7s\n",
      "[CV] END ......................................C=10, gamma=1; total time=   0.7s\n",
      "[CV] END ......................................C=10, gamma=1; total time=   0.7s\n",
      "[CV] END ..................................C=100, gamma=0.01; total time=   0.6s\n",
      "[CV] END ......................................C=10, gamma=1; total time=   0.7s\n",
      "[CV] END ..................................C=100, gamma=0.01; total time=   0.6s\n",
      "[CV] END ..................................C=100, gamma=0.01; total time=   0.7s\n",
      "[CV] END ..................................C=100, gamma=0.01; total time=   0.6s\n",
      "[CV] END ..................................C=100, gamma=0.01; total time=   0.6s\n",
      "[CV] END ..................................C=100, gamma=0.05; total time=   0.6s\n",
      "[CV] END ..................................C=100, gamma=0.05; total time=   0.6s\n",
      "[CV] END ..................................C=100, gamma=0.05; total time=   0.7s\n",
      "[CV] END ..................................C=100, gamma=0.05; total time=   0.7s\n",
      "[CV] END ..................................C=100, gamma=0.05; total time=   0.7s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   0.6s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   0.8s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   0.7s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   0.6s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   0.7s\n",
      "[CV] END ...................................C=100, gamma=0.5; total time=   0.7s\n",
      "[CV] END ...................................C=100, gamma=0.5; total time=   0.7s\n",
      "[CV] END ...................................C=100, gamma=0.5; total time=   0.7s\n",
      "[CV] END ...................................C=100, gamma=0.5; total time=   0.7s\n",
      "[CV] END ...................................C=100, gamma=0.5; total time=   0.7s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   0.6s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   0.7s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   0.6s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   0.6s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   0.6s\n",
      "scikit-learn implementation training time: 11.541383266448975 seconds\n",
      "Best parameters: {'C': 10, 'gamma': 0.01}\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.788\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': [0.01, 0.05, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Initialize SVM with RBF kernel\n",
    "svc = SVC(kernel='rbf')\n",
    "\n",
    "# Measure training time for scikit-learn implementation\n",
    "start_time = time.time()\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "sklearn_training_time = time.time() - start_time\n",
    "print(f\"scikit-learn implementation training time: {sklearn_training_time} seconds\")\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "best_svc = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on training and test data\n",
    "train_accuracy = best_svc.score(X_train, y_train)\n",
    "test_accuracy = best_svc.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[CV] END .................................C=0.01, gamma=0.01; total time=   3.0s\n",
      "[CV] END .................................C=0.01, gamma=0.01; total time=   3.1s\n",
      "[CV] END .................................C=0.01, gamma=0.05; total time=   3.1s\n",
      "[CV] END .................................C=0.01, gamma=0.01; total time=   3.1s\n",
      "[CV] END .................................C=0.01, gamma=0.05; total time=   3.1s\n",
      "[CV] END .................................C=0.01, gamma=0.01; total time=   3.1s\n",
      "[CV] END .................................C=0.01, gamma=0.01; total time=   3.1s\n",
      "[CV] END .................................C=0.01, gamma=0.05; total time=   3.1s\n",
      "[CV] END .................................C=0.01, gamma=0.05; total time=   3.2s\n",
      "[CV] END ..................................C=0.01, gamma=0.1; total time=   3.2s\n",
      "[CV] END ..................................C=0.01, gamma=0.1; total time=   3.2s\n",
      "[CV] END ..................................C=0.01, gamma=0.1; total time=   3.2s\n",
      "[CV] END .................................C=0.01, gamma=0.05; total time=   3.3s\n",
      "[CV] END ..................................C=0.01, gamma=0.5; total time=   3.2s\n",
      "[CV] END ..................................C=0.01, gamma=0.1; total time=   3.3s\n",
      "[CV] END ..................................C=0.01, gamma=0.1; total time=   3.3s\n",
      "[CV] END ..................................C=0.01, gamma=0.5; total time=   3.1s\n",
      "[CV] END ....................................C=0.01, gamma=1; total time=   3.1s\n",
      "[CV] END ..................................C=0.01, gamma=0.5; total time=   3.1s\n",
      "[CV] END ..................................C=0.01, gamma=0.5; total time=   3.1s\n",
      "[CV] END ....................................C=0.01, gamma=1; total time=   3.1s\n",
      "[CV] END ....................................C=0.01, gamma=1; total time=   3.1s\n",
      "[CV] END ....................................C=0.01, gamma=1; total time=   3.1s\n",
      "[CV] END ..................................C=0.01, gamma=0.5; total time=   3.3s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   3.0s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   3.0s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   3.1s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   3.1s\n",
      "[CV] END ....................................C=0.01, gamma=1; total time=   3.2s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   3.1s\n",
      "[CV] END ..................................C=0.1, gamma=0.05; total time=   3.0s\n",
      "[CV] END ..................................C=0.1, gamma=0.05; total time=   3.0s\n",
      "[CV] END ..................................C=0.1, gamma=0.05; total time=   2.9s\n",
      "[CV] END ..................................C=0.1, gamma=0.05; total time=   3.0s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   3.0s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   3.0s\n",
      "[CV] END ..................................C=0.1, gamma=0.05; total time=   3.1s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   3.0s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   3.1s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   3.2s\n",
      "[CV] END ...................................C=0.1, gamma=0.5; total time=   3.1s\n",
      "[CV] END ...................................C=0.1, gamma=0.5; total time=   3.1s\n",
      "[CV] END ...................................C=0.1, gamma=0.5; total time=   3.0s\n",
      "[CV] END ...................................C=0.1, gamma=0.5; total time=   3.0s\n",
      "[CV] END ...................................C=0.1, gamma=0.5; total time=   3.1s\n",
      "[CV] END .....................................C=0.1, gamma=1; total time=   3.1s\n",
      "[CV] END .....................................C=0.1, gamma=1; total time=   3.0s\n",
      "[CV] END .....................................C=0.1, gamma=1; total time=   3.1s\n",
      "[CV] END .....................................C=0.1, gamma=1; total time=   3.1s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   3.0s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   3.0s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   3.0s\n",
      "[CV] END .....................................C=0.1, gamma=1; total time=   3.1s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   3.0s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   3.0s\n",
      "[CV] END ....................................C=1, gamma=0.05; total time=   3.1s\n",
      "[CV] END ....................................C=1, gamma=0.05; total time=   3.0s\n",
      "[CV] END ....................................C=1, gamma=0.05; total time=   3.0s\n",
      "[CV] END ....................................C=1, gamma=0.05; total time=   3.0s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   3.0s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   3.0s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   3.1s\n",
      "[CV] END ....................................C=1, gamma=0.05; total time=   3.3s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   3.1s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   3.2s\n",
      "[CV] END .....................................C=1, gamma=0.5; total time=   3.2s\n",
      "[CV] END .....................................C=1, gamma=0.5; total time=   3.2s\n",
      "[CV] END .....................................C=1, gamma=0.5; total time=   3.3s\n",
      "[CV] END .....................................C=1, gamma=0.5; total time=   3.2s\n",
      "[CV] END .....................................C=1, gamma=0.5; total time=   3.3s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   3.1s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   3.2s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   3.0s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   2.9s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   2.9s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   3.0s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   2.9s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   2.9s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   3.0s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   2.9s\n",
      "[CV] END ...................................C=10, gamma=0.05; total time=   2.9s\n",
      "[CV] END ...................................C=10, gamma=0.05; total time=   2.9s\n",
      "[CV] END ...................................C=10, gamma=0.05; total time=   3.0s\n",
      "[CV] END ...................................C=10, gamma=0.05; total time=   2.9s\n",
      "[CV] END ...................................C=10, gamma=0.05; total time=   3.0s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   3.0s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   2.9s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   2.8s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   3.0s\n",
      "[CV] END ....................................C=10, gamma=0.5; total time=   3.0s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   3.0s\n",
      "[CV] END ....................................C=10, gamma=0.5; total time=   3.0s\n",
      "[CV] END ....................................C=10, gamma=0.5; total time=   3.0s\n",
      "[CV] END ....................................C=10, gamma=0.5; total time=   3.0s\n",
      "[CV] END ....................................C=10, gamma=0.5; total time=   3.1s\n",
      "[CV] END ......................................C=10, gamma=1; total time=   3.0s\n",
      "[CV] END ......................................C=10, gamma=1; total time=   3.0s\n",
      "[CV] END ......................................C=10, gamma=1; total time=   3.0s\n",
      "[CV] END ..................................C=100, gamma=0.01; total time=   2.8s\n",
      "[CV] END ......................................C=10, gamma=1; total time=   3.0s\n",
      "[CV] END ......................................C=10, gamma=1; total time=   3.0s\n",
      "[CV] END ..................................C=100, gamma=0.01; total time=   2.9s\n",
      "[CV] END ..................................C=100, gamma=0.01; total time=   3.0s\n",
      "[CV] END ..................................C=100, gamma=0.01; total time=   2.9s\n",
      "[CV] END ..................................C=100, gamma=0.05; total time=   3.0s\n",
      "[CV] END ..................................C=100, gamma=0.05; total time=   3.0s\n",
      "[CV] END ..................................C=100, gamma=0.05; total time=   3.0s\n",
      "[CV] END ..................................C=100, gamma=0.05; total time=   2.9s\n",
      "[CV] END ..................................C=100, gamma=0.01; total time=   3.0s\n",
      "[CV] END ..................................C=100, gamma=0.05; total time=   3.0s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   3.0s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   3.0s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   3.0s\n",
      "[CV] END ...................................C=100, gamma=0.5; total time=   3.0s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   3.1s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   3.1s\n",
      "[CV] END ...................................C=100, gamma=0.5; total time=   3.0s\n",
      "[CV] END ...................................C=100, gamma=0.5; total time=   3.2s\n",
      "[CV] END ...................................C=100, gamma=0.5; total time=   3.0s\n",
      "[CV] END ...................................C=100, gamma=0.5; total time=   3.0s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   2.1s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   2.1s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   2.2s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   2.2s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   2.1s\n",
      "scikit-learn implementation training time: 49.60028696060181 seconds\n",
      "Best parameters: {'C': 10, 'gamma': 0.01}\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.762\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': [0.01, 0.05, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Initialize SVM with RBF kernel\n",
    "svc = SVC(kernel='rbf')\n",
    "\n",
    "# Measure training time for scikit-learn implementation\n",
    "start_time = time.time()\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "sklearn_training_time = time.time() - start_time\n",
    "print(f\"scikit-learn implementation training time: {sklearn_training_time} seconds\")\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "best_svc = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on training and test data\n",
    "train_accuracy = best_svc.score(X_train, y_train)\n",
    "test_accuracy = best_svc.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "# Extract support vectors\n",
    "support_vectors = best_svc.support_\n",
    "support_vectors_X = X_train[support_vectors]\n",
    "support_vectors_y = np.array(y_train)[support_vectors]\n",
    "\n",
    "# Visualize support vectors\n",
    "fig, axes = plt.subplots(3, 5, figsize=(10, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(15):\n",
    "    axes[i].imshow(support_vectors_X[i].reshape(28, 28), cmap='gray')\n",
    "    axes[i].set_title(f\"Digit: {support_vectors_y[i]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Support Vectors')\n",
    "plt.show()\n",
    "\n",
    "# Visualize some non-support vectors for comparison\n",
    "non_support_indices = np.setdiff1d(np.arange(len(X_train)), support_vectors)\n",
    "non_support_vectors_X = X_train[non_support_indices][:15]\n",
    "non_support_vectors_y = np.array(y_train)[non_support_indices][:15]\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(10, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(15):\n",
    "    axes[i].imshow(non_support_vectors_X[i].reshape(28, 28), cmap='gray')\n",
    "    axes[i].set_title(f\"Digit: {non_support_vectors_y[i]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Non-Support Vectors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
